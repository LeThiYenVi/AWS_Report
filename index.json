[{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/3-blogstranslated/3.1-blog1/","title":"Blog 1","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/3-blogstranslated/3.2-blog2/","title":"Blog 2","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/3-blogstranslated/3.3-blog3/","title":"Blog 3","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/3-blogstranslated/3.4-blog4/","title":"Blog 4","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/3-blogstranslated/3.5-blog5/","title":"Blog 5","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/3-blogstranslated/3.6-blog6/","title":"Blog 6","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nGetting Started with Healthcare Data Lakes: Using Microservices Data lakes can help hospitals and healthcare facilities turn data into business insights, maintain business continuity, and protect patient privacy. A data lake is a centralized, managed, and secure repository to store all your data, both in its raw and processed forms for analysis. Data lakes allow you to break down data silos and combine different types of analytics to gain insights and make better business decisions.\nThis blog post is part of a larger series on getting started with setting up a healthcare data lake. In my final post of the series, ‚ÄúGetting Started with Healthcare Data Lakes: Diving into Amazon Cognito‚Äù, I focused on the specifics of using Amazon Cognito and Attribute Based Access Control (ABAC) to authenticate and authorize users in the healthcare data lake solution. In this blog, I detail how the solution evolved at a foundational level, including the design decisions I made and the additional features used. You can access the code samples for the solution in this Git repo for reference.\nArchitecture Guidance The main change since the last presentation of the overall architecture is the decomposition of a single service into a set of smaller services to improve maintainability and flexibility. Integrating a large volume of diverse healthcare data often requires specialized connectors for each format; by keeping them encapsulated separately as microservices, we can add, remove, and modify each connector without affecting the others. The microservices are loosely coupled via publish/subscribe messaging centered in what I call the ‚Äúpub/sub hub.‚Äù\nThis solution represents what I would consider another reasonable sprint iteration from my last post. The scope is still limited to the ingestion and basic parsing of HL7v2 messages formatted in Encoding Rules 7 (ER7) through a REST interface.\nThe solution architecture is now as follows:\nFigure 1. Overall architecture; colored boxes represent distinct services.\nWhile the term microservices has some inherent ambiguity, certain traits are common:\nSmall, autonomous, loosely coupled Reusable, communicating through well-defined interfaces Specialized to do one thing well Often implemented in an event-driven architecture When determining where to draw boundaries between microservices, consider:\nIntrinsic: technology used, performance, reliability, scalability Extrinsic: dependent functionality, rate of change, reusability Human: team ownership, managing cognitive load Technology Choices and Communication Scope Communication scope Technologies / patterns to consider Within a single microservice Amazon Simple Queue Service (Amazon SQS), AWS Step Functions Between microservices in a single service AWS CloudFormation cross-stack references, Amazon Simple Notification Service (Amazon SNS) Between services Amazon EventBridge, AWS Cloud Map, Amazon API Gateway The Pub/Sub Hub Using a hub-and-spoke architecture (or message broker) works well with a small number of tightly related microservices.\nEach microservice depends only on the hub Inter-microservice connections are limited to the contents of the published message Reduces the number of synchronous calls since pub/sub is a one-way asynchronous push Drawback: coordination and monitoring are needed to avoid microservices processing the wrong message.\nCore Microservice Provides foundational data and communication layer, including:\nAmazon S3 bucket for data Amazon DynamoDB for data catalog AWS Lambda to write messages into the data lake and catalog Amazon SNS topic as the hub Amazon S3 bucket for artifacts such as Lambda code Only allow indirect write access to the data lake through a Lambda function ‚Üí ensures consistency.\nFront Door Microservice Provides an API Gateway for external REST interaction Authentication \u0026amp; authorization based on OIDC via Amazon Cognito Self-managed deduplication mechanism using DynamoDB instead of SNS FIFO because: SNS deduplication TTL is only 5 minutes SNS FIFO requires SQS FIFO Ability to proactively notify the sender that the message is a duplicate Staging ER7 Microservice Lambda ‚Äútrigger‚Äù subscribed to the pub/sub hub, filtering messages by attribute Step Functions Express Workflow to convert ER7 ‚Üí JSON Two Lambdas: Fix ER7 formatting (newline, carriage return) Parsing logic Result or error is pushed back into the pub/sub hub New Features in the Solution 1. AWS CloudFormation Cross-Stack References Example outputs in the core microservice:\nOutputs: Bucket: Value: !Ref Bucket Export: Name: !Sub ${AWS::StackName}-Bucket ArtifactBucket: Value: !Ref ArtifactBucket Export: Name: !Sub ${AWS::StackName}-ArtifactBucket Topic: Value: !Ref Topic Export: Name: !Sub ${AWS::StackName}-Topic Catalog: Value: !Ref Catalog Export: Name: !Sub ${AWS::StackName}-Catalog CatalogArn: Value: !GetAtt Catalog.Arn Export: Name: !Sub ${AWS::StackName}-CatalogArn "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.3-s3-vpc/5.3.1-create-gwe/","title":"Create a gateway endpoint","tags":[],"description":"","content":" Open the Amazon VPC console In the navigation pane, choose Endpoints, then click Create Endpoint: You will see 6 existing VPC endpoints that support AWS Systems Manager (SSM). These endpoints were deployed automatically by the CloudFormation Templates for this workshop.\nIn the Create endpoint console: Specify name of the endpoint: s3-gwe In service category, choose AWS services In Services, type s3 in the search box and choose the service with type gateway For VPC, select VPC Cloud from the drop-down. For Configure route tables, select the route table that is already associated with two subnets (note: this is not the main route table for the VPC, but a second route table created by CloudFormation). For Policy, leave the default option, Full Access, to allow full access to the service. You will deploy a VPC endpoint policy in a later lab module to demonstrate restricting access to S3 buckets based on policies. Do not add a tag to the VPC endpoint at this time. Click Create endpoint, then click x after receiving a successful creation message. "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/4-eventparticipated/4.1-event1/","title":"Event 1","tags":[],"description":"","content":"EVENT HARVEST REPORT Event: Vietnam Cloud Day 2025: Ho Chi Minh City Connect Edition for Builders\nTrack 1: GenAI \u0026amp; Data\n1. ATTENDANCE OBJECTIVES Event participation focused on 4 core objectives to enhance technical capabilities and strategic thinking:\nGenAI Security: Understanding security mechanisms in GenAI and AI Agents to ensure enterprise data safety. Development Process (AI-DLC): Exploring the AI-Driven Development Lifecycle model and how to integrate it into current DevOps processes. Data Platform: Methods for building a Unified Data Platform optimized for Analytics and AI. Trend Updates: Grasping strategies, vision, and latest GenAI technologies on AWS. 2. SPEAKER LIST (AWS EXPERTS) The event brought together top experts from Amazon Web Services:\nJun Kai Loke ‚Äì AI/ML Specialist SA Kien Nguyen ‚Äì Solutions Architect Tamelly Lim ‚Äì Storage Specialist SA Binh Tran ‚Äì Senior Solutions Architect Taiki Dang ‚Äì Solutions Architect Michael Armentano ‚Äì Principal WW GTM Specialist 3. TECHNICAL CONTENT HIGHLIGHTS 3.1. Unified Data Platform Data is the core foundation (\u0026ldquo;Data is the fuel\u0026rdquo;) for successful AI deployment.\nEnd-to-End Architecture: Complete workflow from Ingestion $\\rightarrow$ Storage $\\rightarrow$ Processing $\\rightarrow$ Access $\\rightarrow$ Governance. Objective: Eliminate \u0026ldquo;Silos\u0026rdquo; (isolation) in data, people, and processes. Moving towards Self-service and standardized Governance. Key AWS Services: S3, Glue, Redshift, Lake Formation, OpenSearch, Kinesis/MSK. Featured Technology: Zero-ETL integration (connecting S3 $\\leftrightarrow$ Redshift, Aurora, DynamoDB\u0026hellip;) helps minimize latency and complexity in data transfer. 3.2. GenAI Strategy \u0026amp; Tools on AWS Amazon Bedrock: Primary service allowing Foundation Models (FM) selection, RAG integration, Guardrails, optimization between cost and latency. Amazon SageMaker Unified Studio: Unified platform for Data, Analytics \u0026amp; AI. Comprehensive MLOps support (Pipelines, Registry, Deployment, Monitoring). AI Agents - Productivity Enhancement: Transition from single Assistant to Multi-agent systems (like AgentCore, Amazon Nova). Supporting frameworks: CrewAI, LangGraph, LlamaIndex. Real-world Applications: Customer service (CS), Business Intelligence (BI) with Amazon Q (in QuickSight), business process automation. 3.3. GenAI Security \u0026amp; Trust Multi-layer Security Model: Protection from Infrastructure $\\rightarrow$ Model $\\rightarrow$ Application. 5 Security Pillars: Compliance, Privacy, Controls, Risk Management, Resilience. Hallucination Mitigation: Combining Prompt Engineering, RAG (Retrieval-Augmented Generation), and Fine-tuning. RAG Workflow: Input $\\rightarrow$ Embedding $\\rightarrow$ Context $\\rightarrow$ LLM $\\rightarrow$ Output. Control Tools: Bedrock Guardrails, Human-in-the-loop, Observability (OpenTelemetry) to prevent risks according to OWASP LLM standards. 3.4. AI-Driven Development Lifecycle (AI-DLC) A new concept in software development lifecycle:\nEvolution: AI-Assisted $\\rightarrow$ AI-Driven $\\rightarrow$ AI-Managed. Stages: Inception $\\rightarrow$ Construction $\\rightarrow$ Operation. Implementation: Integrating Infrastructure as Code (IaC), automated testing, monitoring, and risk management directly into the process. 4. KEY TAKEAWAYS \u0026amp; NEW THINKING After the event, I synthesized important changes in thinking and strategy:\nSystem Design Thinking:\nMust design Data \u0026amp; AI systems in an End-to-end approach, firmly eliminating Data Silos. Apply Self-service and Governance principles from the early stages. Technical Architecture:\nLeverage the power of Zero-ETL and Lakehouse for sustainable, easily scalable system operations. Combine service pieces (S3, Glue, Bedrock\u0026hellip;) into a unified platform instead of disparate tools. Development Strategy:\nBusiness-first: All technology decisions must originate from business problems. Data and Security are the \u0026ldquo;Foundation\u0026rdquo; - without this foundation, AI cannot deliver value. Balance: Between innovation speed and cost/security. AI-DLC: Standardize development processes with AI participation at every stage (Code, Review, Test, Doc). 5. APPLICATION PLAN FOR WORK Based on learned knowledge, I propose specific application plans:\nIn Real Projects:\nPilot AI Agent for Login/Registration module and Customer Support (Chatbot). Integrate Validation/Guardrails to ensure GenAI input/output data safety. In Learning \u0026amp; Teamwork:\nApply AI-DLC model: Use AI to assist in writing boilerplate code and creating technical documentation; humans focus on Review and Approve. Clearly analyze when to use Serverless (Lambda) (for short tasks, events) and when to use Containers (ECS/Fargate) (for long-term, complex applications). In Personal Role:\nPractice documentation thinking and requirements gathering in a Business-first approach. Focus on building a solid Data Foundation before deploying complex AI models. 6. EVENT \u0026amp; WORKSHOP EXPERIENCE (Recording actual experience at the event)\nThe most memorable highlight was the \u0026ldquo;GenAI-powered App-DB Modernization\u0026rdquo; Workshop. This was a valuable opportunity to practice modernizing applications and databases:\nTechnical Experience: Hands-on designing end-to-end data pipelines and accessing latest tools like Amazon Bedrock, AgentCore. RAG Practice: Deep understanding of how to minimize Hallucination through coordinating Prompt Engineering and RAG workflow. Expert Connection: Direct exchanges with AWS engineers about real Case Studies, helping strengthen confidence in Multi-agent models and AI-DLC. Conclusion: GenAI is not just a trend tool, but requires a comprehensive strategy from Infrastructure Architecture, Data to Security to truly bring value to enterprises.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/","title":"Internship Report","tags":[],"description":"","content":"Internship Report ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your report, including this warning.\nStudent Information: Full Name: Le Thi Yen Vi\nPhone Number: 0985868349\nEmail: viltyse182544@fpt.edu.vn\nUniversity: FPT University\nMajor: Software Engineering\nClass: AWS082025\nInternship Company: Amazon Web Services Vietnam Co., Ltd.\nInternship Position: FCJ Cloud Intern\nInternship Duration: From 08/09/2025 to 12/12/2025\nReport Content Worklog Proposal Translated Blogs Events Participated Workshop Self-evaluation Sharing and Feedback "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.4-s3-onprem/5.4.1-prepare/","title":"Prepare the environment","tags":[],"description":"","content":"To prepare for this part of the workshop you will need to:\nDeploying a CloudFormation stack Modifying a VPC route table. These components work together to simulate on-premises DNS forwarding and name resolution.\nDeploy the CloudFormation stack The CloudFormation template will create additional services to support an on-premises simulation:\nOne Route 53 Private Hosted Zone that hosts Alias records for the PrivateLink S3 endpoint One Route 53 Inbound Resolver endpoint that enables \u0026ldquo;VPC Cloud\u0026rdquo; to resolve inbound DNS resolution requests to the Private Hosted Zone One Route 53 Outbound Resolver endpoint that enables \u0026ldquo;VPC On-prem\u0026rdquo; to forward DNS requests for S3 to \u0026ldquo;VPC Cloud\u0026rdquo; Click the following link to open the AWS CloudFormation console. The required template will be pre-loaded into the menu. Accept all default and click Create stack. It may take a few minutes for stack deployment to complete. You can continue with the next step without waiting for the deployemnt to finish.\nUpdate on-premise private route table This workshop uses a strongSwan VPN running on an EC2 instance to simulate connectivty between an on-premises datacenter and the AWS cloud. Most of the required components are provisioned before your start. To finalize the VPN configuration, you will modify the \u0026ldquo;VPC On-prem\u0026rdquo; routing table to direct traffic destined for the cloud to the strongSwan VPN instance.\nOpen the Amazon EC2 console\nSelect the instance named infra-vpngw-test. From the Details tab, copy the Instance ID and paste this into your text editor\nNavigate to the VPC menu by using the Search box at the top of the browser window.\nClick on Route Tables, select the RT Private On-prem route table, select the Routes tab, and click Edit Routes.\nClick Add route. Destination: your Cloud VPC cidr range Target: ID of your infra-vpngw-test instance (you saved in your editor at step 1) Click Save changes "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.1-week1/","title":"Week 1 Worklog","tags":[],"description":"","content":"Week 1 Objectives: Connect and get acquainted with members of First Cloud Journey. Understand the core AWS foundational services, including Account, Billing, IAM, VPC, and EC2. Learn how to manage costs and access permissions. Tasks to be carried out this week: Day Task Start Date Completion Date Reference Material 2 - Get acquainted with FCJ members - Read and take note of internship unit rules and regulations - Explore AWS Services: Get a high-level overview of the main service categories. 09/08/2025 09/08/2025 3 - Learn about Creating Your First AWS Account - Learn about Managing Costs with AWS Budgets - Learn about Getting Help with AWS Support - Practice: + Create AWS Free Tier account + Set up a budget alert + \u0026hellip; 09/09/2025 09/09/2025 https://cloudjourney.awsstudygroup.com/ 4 - Learn about Access Management with AWS Identity and Access Management (IAM) - Practice: + Create an IAM User, Group + Attach policies for access 09/10/2025 09/10/2025 https://cloudjourney.awsstudygroup.com/ 5 - Learn about Networking Essentials with Amazon Virtual Private Cloud (VPC) - Learn about Compute Essentials with Amazon Elastic Compute Cloud (EC2) - Practice: + Set up a custom VPC\n+ Launch an EC2 instance into the VPC 09/11/2025 09/11/2025 https://cloudjourney.awsstudygroup.com/ 6 - Learn about Instance Profiling with IAM Roles for EC2 - Practice: + Create an IAM Role for EC2 + Attach the Role to an existing EC2 instance 09/12/2025 09/12/2025 https://cloudjourney.awsstudygroup.com/ Week 1 Achievements: Understood the foundational AWS service groups:\nCompute (Amazon EC2) Networking (Amazon VPC) Identity \u0026amp; Access Management (IAM) Billing \u0026amp; Cost Management (AWS Budgets) \u0026hellip; Successfully created and configured an AWS Free Tier account.\nBecame familiar with the AWS Management Console and learned how to find, access, and use services via the web interface.\nPracticed with Identity and Access Management (IAM), including:\nCreating Users and Groups Attaching Policies Creating IAM Roles for EC2 \u0026hellip; Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/","title":"Worklog","tags":[],"description":"","content":"This is a detailed worklog of the First Cloud Journey (FCJ) Workforce program completed over 12 weeks (84 days). Each week focuses on different aspects of AWS Cloud, from fundamental basics to advanced services, helping build a solid foundation for the AWS Cloud Engineer role.\n12-Week Learning Roadmap: Week 1: AWS Foundations - Getting Started with Account, IAM, VPC and EC2\nCreate AWS account, configure IAM, understand VPC and EC2 basics Week 2: Security and Networking - Security Foundations \u0026amp; Networking\nSet up IAM Users/Groups, MFA, build Custom VPC with Public/Private Subnets Week 3: Compute and Application Identity - EC2 \u0026amp; Instance Profiling\nLaunch EC2, use IAM Roles, manage EBS Volumes Week 4: Storage and Database - Cloud9, S3 \u0026amp; RDS\nSet up Cloud9 environment, deploy S3 Static Website, initialize RDS MySQL Week 5: Scalability - Lightsail \u0026amp; Auto Scaling\nGet familiar with Docker, deploy Application Load Balancer and Auto Scaling Groups Week 6: Monitoring and Automation - CloudWatch \u0026amp; Lambda\nBuild Dashboard, create Alarms, write Lambda functions for automation Week 7: Systems Management - AWS Systems Manager\nUse SSM Session Manager, Run Command, manage fleet with Tagging Week 8: Infrastructure as Code - CloudFormation \u0026amp; CDK\nLearn CloudFormation, deploy infrastructure with AWS CDK (TypeScript) Week 9: Network and Cost Optimization - VPC Flow Logs \u0026amp; Cost Optimization\nAnalyze network traffic, use Cost Explorer and Compute Optimizer Week 10: Serverless Architecture - API Gateway, Lambda \u0026amp; DynamoDB\nBuild complete RESTful API with serverless architecture Week 11: Governance and Compliance - Governance \u0026amp; Well-Architected Review\nAudit infrastructure, manage Service Quotas, apply Well-Architected Framework Week 12: Capstone Project and Career Preparation - Career Readiness\nDeploy comprehensive project, practice SAA-C03 certification exam, prepare portfolio "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.1-workshop-overview/","title":"Workshop Overview","tags":[],"description":"","content":"Introduction to EV Rental AI Agent What is an AI Agent? An AI Agent is an intelligent system that can:\nUnderstand natural language queries Automatically select and execute appropriate tools/functions Make decisions based on context Provide structured responses with data Unlike traditional chatbots with fixed responses, AI Agents can reason and take actions dynamically.\nSystem Architecture Our EV Rental AI Agent uses a multi-layered architecture:\n‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ User Interface ‚îÇ ‚Üê React Frontend (Chat UI) ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ HTTP/REST ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ FastAPI Server ‚îÇ ‚Üê Backend Orchestrator ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚Üì ‚Üì ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ Strands ‚îÇ ‚îÇ PostgreSQL ‚îÇ ‚îÇ Agent SDK‚îÇ ‚îÇ (History) ‚îÇ ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí AWS Bedrock (Claude 3.5 Sonnet) ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Knowledge Base (Policies/FAQ) ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí Backend API (Vehicles/Stations) Key Components Component Technology Purpose AI Model AWS Bedrock - Claude 3.5 Sonnet Natural language understanding \u0026amp; generation Agent Framework Strands Agent SDK Automatic tool selection \u0026amp; orchestration Backend API FastAPI (Python) REST API server for agent logic Database PostgreSQL Store chat history \u0026amp; sessions Frontend React + Chakra UI Interactive chat interface Knowledge Base AWS Bedrock KB Document retrieval (policies, FAQ) Core Features 1. Knowledge Base Search Agent searches through uploaded documents to answer questions about:\nRental policies Pricing information Booking procedures Terms and conditions Example Query:\n\u0026ldquo;Ch√≠nh s√°ch thu√™ xe c·ªßa b·∫°n l√† g√¨?\u0026rdquo;\nAgent Response:\n## üìã VinFast Rental Policies ### üìÑ Required Documents: - ‚úÖ Valid ID/Passport - ‚úÖ Driver\u0026#39;s License (Class B1+) - ‚úÖ Proof of Residence ### üí∞ Pricing: - **VF8**: 1,500,000 VNƒê/day - **VF9**: 2,000,000 VNƒê/day - **Deposit**: 10,000,000 VNƒê 2. Vehicle Search Agent queries the backend API to find available vehicles based on:\nLocation (city) Date range Vehicle model/type Response Format: Interactive vehicle cards with specs\n3. Charging Station Finder Agent retrieves nearby charging stations with:\nAddress and status Available chargers Distance (if location provided) Response Format: Station cards with real-time availability\nWorkshop Objectives By the end of this workshop, you will be able to:\n‚úÖ Configure AWS Bedrock - Enable Claude models and create a Knowledge Base ‚úÖ Build an AI Agent Backend - Use Strands SDK to orchestrate multiple tools ‚úÖ Deploy a Chat Interface - Create a responsive React chat UI ‚úÖ Test End-to-End - Interact with the AI agent and verify all functionalities Technology Stack AWS Services:\nAWS Bedrock (Claude 3.5 Sonnet v2) AWS Bedrock Knowledge Bases AWS S3 (for document storage) IAM (for access management) Backend:\nPython 3.11+ FastAPI Strands Agent SDK PostgreSQL SQLAlchemy Frontend:\nReact 18 Chakra UI Axios React Markdown Workshop Flow Step 1: Prerequisites ‚Üì Step 2: Setup AWS Bedrock \u0026amp; Knowledge Base ‚Üì Step 3: Deploy Backend API (FastAPI) ‚Üì Step 4: Deploy Frontend (React) ‚Üì Step 5: Test the AI Agent ‚Üì Step 6: Cleanup Resources Next: Let\u0026rsquo;s move to Prerequisites to prepare your environment.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.4-s3-onprem/5.4.2-create-interface-enpoint/","title":"Create an S3 Interface endpoint","tags":[],"description":"","content":"In this section you will create and test an S3 interface endpoint using the simulated on-premises environment deployed as part of this workshop.\nReturn to the Amazon VPC menu. In the navigation pane, choose Endpoints, then click Create Endpoint.\nIn Create endpoint console:\nName the interface endpoint In Service category, choose aws services In the Search box, type S3 and press Enter. Select the endpoint named com.amazonaws.us-east-1.s3. Ensure that the Type column indicates Interface. For VPC, select VPC Cloud from the drop-down. Make sure to choose \u0026ldquo;VPC Cloud\u0026rdquo; and not \u0026ldquo;VPC On-prem\u0026rdquo;\nExpand Additional settings and ensure that Enable DNS name is not selected (we will use this in the next part of the workshop) Select 2 subnets in the following AZs: us-east-1a and us-east-1b For Security group, choose SGforS3Endpoint: Keep the default policy - full access and click Create endpoint Congratulation on successfully creating S3 interface endpoint. In the next step, we will test the interface endpoint.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/4-eventparticipated/4.2-event2/","title":"Event 2","tags":[],"description":"","content":"EVENT HARVEST REPORT Event: AWS Cloud Mastery Series #1: Generative AI, RAG \u0026amp; AWS Agentic AI\n1. ATTENDANCE OBJECTIVES Event participation focused on 5 core objectives to enhance technical capabilities and strategic thinking:\nPrompt Engineering: Master Prompt Engineering techniques to optimize the ability to control language models. AWS AI Services: Explore the ecosystem of AWS Pretrained AI Services. RAG Architecture: Dive deep into building intelligent applications using RAG (Retrieval-Augmented Generation) techniques. Agentic AI: Update on Agentic AI trends and methods for transitioning AI Agents from Proof of Concept (POC) to Production through Amazon Bedrock AgentCore. Voice AI Framework: Approach Pipecat ‚Äì Framework supporting real-time voice interactive virtual assistant development. 2. SPEAKER LIST The event brought together top experts in AI and Cloud:\nLam Tuan Kiet ‚Äì Sr DevOps Engineer (FPT Software) Danh Hoang Hieu Nghi ‚Äì AI Engineer (Renova Cloud) Dinh Le Hoang Anh ‚Äì Cloud Engineer Trainee (First Cloud AI Journey) 3. TECHNICAL CONTENT HIGHLIGHTS 3.1. Prompt Engineering \u0026amp; Foundation Models (Core Foundation) Before approaching advanced services, the event emphasized the importance of effective communication with Foundation Models on Amazon Bedrock:\nZero-shot / Few-shot Prompting: Techniques for direct commands or providing a few sample examples to guide desired output. Chain of Thought (CoT): Requiring the model to \u0026ldquo;reason step-by-step\u0026rdquo;, enhancing accuracy when solving complex logic problems. 3.2. Pretrained AI Services (AWS AI Services) Introduction to convenient API suite, allowing AI integration without the effort of model training:\nImage/Video: Amazon Rekognition. Language: Amazon Translate, Comprehend, Textract (OCR). Audio: Amazon Polly (Text-to-Speech), Transcribe (Speech-to-Text). 3.3. RAG - Retrieval Augmented Generation Solution helping AI respond based on enterprise internal data, minimizing the risk of providing incorrect information:\nEmbeddings: Applying Amazon Titan Text Embeddings V2 to vectorize data for semantic search. Knowledge Bases for Amazon Bedrock: Managing the entire process from data Chunking $\\rightarrow$ Vector Storage $\\rightarrow$ Retrieval $\\rightarrow$ Answer Generation. 3.4. Evolution to Agentic AI (The Era of Task AI) The evolution picture of GenAI:\nGenAI Assistants: Supporting automation of repetitive rule-based tasks. GenAI Agents: Goal-oriented, handling broader workflow chains. Agentic AI Systems: Multi-agent systems operating completely autonomously with minimal human intervention. \u0026ldquo;The Prototype to Production Chasm\u0026rdquo; Challenge: Bringing Agents to market faces major barriers in Performance, Security \u0026amp; Governance, and Complexity in context management/auditing. 3.5. Amazon Bedrock AgentCore: Solution for Bringing Agents to Market AWS introduces AgentCore to solve Agent operations challenges:\nCore Components: Runtime \u0026amp; Memory (Execution environment \u0026amp; Memory), Identity \u0026amp; Gateway (Identity \u0026amp; Connection), Code Interpreter (Self-write and run code), Observability (Operations monitoring). Benefits: Helps developers focus on business logic instead of struggling with security infrastructure or context storage. 3.6. Pipecat: Framework for Real-time Voice AI Introduction to open-source framework optimized for Multimodal virtual assistants:\nFeatures: Optimized for low latency (Real-time) and streaming processing. Operating mechanism: WebRTC Input (Receive audio) $\\rightarrow$ STT (Convert to text) $\\rightarrow$ LLM Processing $\\rightarrow$ TTS (Convert to speech) $\\rightarrow$ Output (Playback). 4. DETAILED EVENT EXPERIENCE The workshop helped me expand my vision from foundational knowledge to pioneering technologies shaping the future of AI.\n4.1. Shift from \u0026ldquo;Q\u0026amp;A\u0026rdquo; to \u0026ldquo;Action\u0026rdquo; (Agentic AI) What impressed me most was the concept of Agentic AI. It changed my thinking about AI from just chat/summarization to the image of \u0026ldquo;virtual employees\u0026rdquo; capable of self-planning, using tools (web browsing, code writing) to solve problems without hand-holding.\n4.2. Solving the \u0026ldquo;Production\u0026rdquo; Problem I highly appreciate the sharing about the \u0026ldquo;Chasm\u0026rdquo; between POC and Production. Tools like Amazon Bedrock AgentCore truly are the solution to enterprise trust issues, thanks to the robust security mechanisms (Identity) and monitoring (Observability) that AWS provides.\n4.3. Voice AI Potential with Pipecat The Pipecat demo was truly interesting. The combination of WebRTC and AI to create smooth conversations with extremely low latency opens up huge application potential: from Virtual Call Centers, Interview Assistants to AI Language Teachers.\n5. CONCLUSION The workshop \u0026ldquo;Generative AI \u0026amp; Agentic AI on AWS\u0026rdquo; painted a comprehensive value picture:\nPresent: We optimize data with RAG and Prompt Engineering. Future: The Agentic AI era is approaching, where Autonomous Agents will change how businesses operate. Tools: With the AWS ecosystem (Bedrock, AgentCore) and Frameworks (Pipecat, LangChain), technical barriers are gradually being removed, creating momentum for engineers to realize breakthrough ideas. "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.2-prerequiste/","title":"Prerequisites","tags":[],"description":"","content":"Prerequisites for EV Rental AI Agent Workshop Before starting this workshop, ensure you have the following requirements ready:\n1. AWS Account You need an AWS Account with appropriate permissions to:\nAccess AWS Bedrock service Create and manage IAM users Create S3 buckets (for Knowledge Base) Create Knowledge Bases Note: Bedrock is available in specific regions. Recommended regions:\nus-west-2 (Oregon) us-east-1 (N. Virginia) ap-southeast-1 (Singapore) 2. IAM User with Bedrock Permissions You need to create an IAM User with AWS Bedrock access for your application.\nStep 1: Create IAM User\nGo to AWS Console ‚Üí IAM ‚Üí Users ‚Üí Create User User name: bedrock-agent-user ‚úÖ Check: Provide user access to the AWS Management Console (optional) ‚úÖ Select: I want to create an IAM user Click Next Step 2: Attach Permissions\nSelect: Attach policies directly Search and select these policies: ‚úÖ AmazonBedrockFullAccess - Full access to Bedrock models and Knowledge Bases ‚úÖ (Optional) AmazonS3ReadOnlyAccess - If using Knowledge Base with S3 Click Next ‚Üí Create User Step 3: Create Access Keys\nClick on the newly created user: bedrock-agent-user Go to Security credentials tab Scroll down to Access keys ‚Üí Click Create access key Select use case: Application running outside AWS Click Next ‚Üí Create access key ‚ö†Ô∏è IMPORTANT: Copy and save: Access Key ID (example: AKIAIOSFODNN7EXAMPLE) Secret Access Key (shown only once, example: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY) Click Done ‚ö†Ô∏è Security Note:\n# Save to .env file (DO NOT commit to Git) AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID_HERE AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY_HERE AWS_REGION=us-west-2 3. Development Environment 3.1. Python Environment Python 3.11 or higher Package manager: pip Verify installation:\npython --version # Expected: Python 3.11.x or higher pip --version 3.2. Node.js Environment Node.js 18+ and npm Required for React frontend Verify installation:\nnode --version # Expected: v18.x.x or higher npm --version 3.3. PostgreSQL Database PostgreSQL 14+ installed locally or use Docker Option 1: Install locally\nDownload from: https://www.postgresql.org/download/ Create database: ev_rental_db Option 2: Use Docker\ndocker run -d \\ --name postgres-ev \\ -e POSTGRES_PASSWORD=password \\ -e POSTGRES_DB=ev_rental_db \\ -p 5432:5432 \\ postgres:14 Verify PostgreSQL:\n# Check PostgreSQL is running psql --version # Connect to database psql -U postgres -d ev_rental_db 4. Code Editor \u0026amp; Tools VS Code or your preferred IDE Git for cloning repositories Postman or cURL for API testing (optional) Install VS Code:\nDownload from: https://code.visualstudio.com/ Install Git:\n# macOS brew install git # Windows # Download from: https://git-scm.com/download/win # Verify git --version 5. AWS CLI (Optional) Install AWS CLI to interact with AWS services from command line:\n# macOS/Linux curl \u0026#34;https://awscli.amazonaws.com/AWSCLIV2.pkg\u0026#34; -o \u0026#34;AWSCLIV2.pkg\u0026#34; sudo installer -pkg AWSCLIV2.pkg -target / # Windows # Download from: https://awscli.amazonaws.com/AWSCLIV2.msi # Verify aws --version Configure AWS CLI:\naws configure # Enter your Access Key ID: AKIA5GPEMGJZK6E7PMEB # Enter your Secret Access Key: (paste your secret key) # Default region name: us-west-2 # Default output format: json Test AWS CLI:\n# List available Bedrock models aws bedrock list-foundation-models --region us-west-2 # Check your identity aws sts get-caller-identity Prerequisites Checklist Before proceeding to the next step, ensure you have:\n‚úÖ AWS Account with Bedrock access in supported region ‚úÖ IAM User created with AmazonBedrockFullAccess policy ‚úÖ Access Key ID and Secret Access Key saved securely ‚úÖ Python 3.11+ installed and verified ‚úÖ Node.js 18+ and npm installed and verified ‚úÖ PostgreSQL 14+ database running ‚úÖ Code editor (VS Code recommended) installed ‚úÖ Git installed and configured ‚úÖ (Optional) AWS CLI installed and configured Estimated Costs This workshop uses the following AWS services:\nService Estimated Cost Notes AWS Bedrock - Claude 3.5 Sonnet ~$0.50 - $2.00 Pay per API call (input/output tokens) AWS Bedrock - Knowledge Base ~$0.10 - $0.50 Vector storage + retrieval S3 Storage ~$0.02 Minimal for documents Data Transfer ~$0.05 Usually within free tier Total ~$0.67 - $2.57 For the entire workshop üí° Tip: Remember to clean up resources after the workshop to avoid ongoing charges!\nNext: Proceed to Setup AWS Bedrock to enable models and create Knowledge Base.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/2-proposal/","title":"Proposal","tags":[],"description":"","content":"The EV Station-based Rental System Electric Vehicle Rental and Return Software at Fixed Stations ‚Äì A Green Mobility Solution for Smart Cities 1. Executive Summary The EV Station-based Rental System is developed to provide an all-in-one platform for electric vehicle rental and charging management. It integrates real-time rental, payment, and charging station access through a unified cloud-native solution. The system features a React Native mobile app and a Spring Boot backend deployed on AWS ECS Fargate, with PostgreSQL (RDS) and Redis (ElastiCache) for data and caching. User authentication is managed via Amazon Cognito, and global delivery is optimized using CloudFront. Designed under the AWS Well-Architected Framework, the platform ensures scalability, high availability, and security while maintaining cost efficiency.\n2. Problem Statement What‚Äôs the Problem? Current electric vehicle (EV) rental services are fragmented, requiring users to switch between multiple apps to locate, book, and manage rentals at fixed points. This creates inconvenience, slow performance, and unreliable experiences ‚Äî users often arrive at ‚Äúunavailable‚Äù or ‚Äúoffline‚Äù rental points, leading to frustration and loss of trust.\nFor vehicle owners and operators, manual fleet management, booking coordination, and maintenance tracking result in operational inefficiencies and lost revenue. Currently, there is no unified, real-time platform connecting renters, vehicle owners, and rental point operators.\nThe Solution The EV Station-based Rental System consolidates EV rental and return at fixed points into a single, cloud-native platform. Built with React Native for mobile and Spring Boot for backend, the system delivers real-time booking, vehicle tracking, and payment integration.\nKey AWS services include ECS Fargate for compute, RDS PostgreSQL for data storage, ElastiCache for low-latency performance, API Gateway and Cognito for secure access, and CloudFront for global content delivery. The platform supports both fleet-based and peer-to-peer (P2P) vehicle registration, providing a centralized interface for users and operators to manage rentals efficiently, securely, and at scale.\nBenefits and Return on Investment The platform eliminates manual coordination and fragmented applications, offering a unified, automated experience for renters and fleet owners. Real-time data ensures reliability and transparency regarding vehicle availability and rental point status.\nDesigned under the AWS Well-Architected Framework, the system minimizes operational costs with a serverless, pay-per-use model while maintaining scalability and 99.99% uptime. Within 12‚Äì24 months, the platform is projected to reach 50,000+ monthly active users, onboard 200+ rental points, and deliver significant time, cost, and operational efficiencies for both users and operators.\n3. Solution Architecture The VoltGo platform adopts a serverless and fully private AWS architecture for secure and scalable backend operations. Backend run on Amazon ECS Fargate, connecting to Aurora PostgreSQL Serverless v2 for relational data and ElastiCache Serverless (Redis) for caching. All workloads are deployed in private subnets across multiple Availability Zones and accessed securely through API Gateway via AWS PrivateLink to an internal Network Load Balancer. User authentication is managed by Amazon Cognito, while the frontend is hosted on Amazon S3 and delivered globally via CloudFront, protected by AWS WAF and ACM SSL/TLS. Monitoring and secrets management are handled by CloudWatch and Secrets Manager, with the entire infrastructure provisioned through Terraform IaC. This architecture ensures high security, elasticity, and cost efficiency suitable for the current development stage and future production scaling.\nAWS Services Used Amazon ECS Fargate: Serverless container orchestration for backend microservices. Amazon Aurora PostgreSQL Serverless v2: Scalable, multi-AZ relational database. Amazon ElastiCache Serverless (Redis): In-memory caching for low-latency data access. Amazon API Gateway: Secure REST API entry point integrated via PrivateLink. Amazon Cognito: User authentication and authorization with JWT and MFA. Amazon CloudFront + S3: Global content delivery and static hosting with WAF protection. AWS Secrets Manager: Centralized secret storage and automatic rotation. Amazon CloudWatch: Unified monitoring, logging, and alerting for all services. AWS WAF + ACM: Edge-level security and SSL/TLS certificate management. Component Design Frontend:React/Vue.js web application hosted on Amazon S3 and delivered via CloudFront, secured with AWS WAF and ACM SSL/TLS certificates. API Layer: Amazon API Gateway provides the public API endpoint, connecting privately to backend services through AWS PrivateLink to an internal Network Load Balancer. Compute Layer: Amazon ECS Fargate runs containerized microservices across multiple Availability Zones, scaling automatically based on CPU and memory utilization. Database Layer:Amazon Aurora PostgreSQL Serverless v2 stores relational data with a writer and read replica for high availability and automated scaling. Caching Layer: Amazon ElastiCache Serverless (Redis) caches session and booking data to reduce database load and improve response time. Authentication: Amazon Cognito handles user registration, login, and JWT-based authorization with optional MFA support. Storage: Amazon S3 manages static assets and user uploads, accessible only through CloudFront via Origin Access Control (OAC). Monitoring \u0026amp; Security: Amazon CloudWatch tracks logs and performance metrics, while AWS Secrets Manager securely stores credentials with automatic rotation. 4. Technical Implementation Implementation Phases This project has two main parts‚Äîdeveloping the backend locally and deploying it to the AWS cloud‚Äîeach following four key phases:\n1.Build and Design Architecture: Develop and test backend services locally using Docker Compose, PostgreSQL, and Redis. Design the AWS serverless architecture including ECS Fargate, Aurora Serverless, ElastiCache, and API Gateway with PrivateLink connections. (Pre-deployment phase) 2.Estimate Cost and Validate Feasibility: Use AWS Pricing Calculator to estimate the monthly cost of ECS tasks, Aurora capacity units, and CloudFront bandwidth. Adjust design decisions to ensure cost-effectiveness and smooth migration. 3.Configure and Deploy Infrastructure: Build and deploy cloud infrastructure using Terraform for IaC. Configure VPC, ECS, Aurora, ElastiCache, Cognito, and CloudFront. Validate IAM roles, networking, and private-only access via VPC Endpoints. 4.Test, Optimize, and Release: Deploy Dockerized services to ECS Fargate, test API Gateway ‚Üí PrivateLink ‚Üí NLB ‚Üí ECS flow, and verify database connections. Enable CloudWatch monitoring, auto-scaling, and WAF protection. Optimize scaling thresholds and document final architecture. Technical Requirements\nBackend Services: Node.js or Spring Boot microservices for Auth, Booking, and Payment, containerized with Docker and deployed to ECS Fargate (2‚Äì10 tasks, auto-scaling). Database Layer: Amazon Aurora PostgreSQL Serverless v2 with writer and reader instances, supporting automatic scaling and multi-AZ high availability. Caching Layer: Amazon ElastiCache Serverless (Redis 7.1) for session caching and frequently accessed data. Authentication: Amazon Cognito manages user registration, JWT-based authentication, and optional MFA, integrated with API Gateway. Storage \u0026amp; Content Delivery: Frontend hosted on Amazon S3 and distributed via CloudFront, protected by AWS WAF and ACM SSL/TLS certificates. Secrets \u0026amp; Monitoring: AWS Secrets Manager for storing credentials (DB, Redis, JWT keys) with 30-day rotation. Amazon CloudWatch for logging, metrics, and scaling alarms. 5. Timeline \u0026amp; Milestones Project Timeline\nPhase 1: Foundation \u0026amp; Design (Weeks 1-2) Week 1: Finalize MVP scope (P0 User Stories), define user flows, and approve the AWS architecture. Week 2: FE Lead finalizes UI/UX mockups. Backend provisions core AWS (VPC, S3, ECR, Aurora). Phase 2: Core MVP Development (Weeks 3-8) Weeks 3-4: Backend builds User Auth (Cognito) \u0026amp; core APIs (API Gateway, ECS). Weeks 5-6: All teams (FE/BE/Mobile) build core screens (Login, Search, Details) and the Booking Engine APIs. Weeks 7-8: Integration of KYC flow (Lambda, Textract, Rekognition) and Payment Gateway integration. Phase 3: Testing \u0026amp; UAT (Weeks 9-10) Week 9: Full End-to-End (E2E) testing. QA is performed by the 5-person dev team, as no dedicated QA is allocated. Week 10: Stakeholder User Acceptance Testing (UAT) and final critical bug fixing. Phase 4: Launch (Week 11) Week 11: Production deployment, Go-live, and intensive Hypercare monitoring via CloudWatch. 6. Budget Estimation This budget estimate is based on the provided AWS architecture diagram and the \u0026ldquo;cheapest possible\u0026rdquo; MVP launch strategy, maximizing Free Tier usage.\nInfrastructure Costs AWS Services (Monthly Estimate): Amazon Route 53: $0.50/month (1 hosted zone). AWS WAF: $6.00/month (1 WebACL + 1 Rule + minimal requests). AWS S3 Standard: $0.00/month (Stays within 5GB Always Free tier). Amazon CloudFront: $0.00/month (Stays within 1TB/10M request Always Free tier). AWS Cognito: $0.00/month (Stays within 10,000 MAU free tier). Amazon API Gateway: $0.00/month (Stays within 1M request 12-month free tier). AWS Lambda: $0.00/month (Stays within 1M request Always Free tier). Amazon Textract/Rekognition: $0.00/month (Stays within 12-month free tier for KYC). Application Load Balancer: $17.52/month (1 ALB, minimal processing). VPC Endpoint (PrivateLink): $7.30/month (1 Endpoint, 1 AZ, 1GB data). Amazon ECS on Fargate: ~$20.00/month (Assumes 2 minimal 24/7 containers, e.g., 0.25 vCPU/0.5GB RAM). Amazon Aurora Serverless v2: ~$25.00/month (Minimal ACUs, configured to scale to near-zero). Amazon ElastiCache Serverless: ~$10.00/month (Minimal usage). Amazon CloudWatch: $0.00/month (Stays within 5GB log Always Free tier). Amazon ECR: ~$0.10/month (Minimal storage over 500MB free tier). Total: ~$86.42/month, ~$1,037.04/12 months\n7. Risk Assessment Risk Matrix System Downtime: High impact, medium probability. Data Sync Errors (Between Stations \u0026amp; Server): Medium impact, high probability. OCR Verification Failure: Medium impact, medium probability. Vehicle Shortage or Low Battery at Stations: High impact, high probability. Operational Mistakes by Staff: Medium impact, medium probability. Cost Overruns: Medium impact, low probability. Mitigation Strategies System: Use load-balanced cloud servers with auto-scaling and failover backup. Data Sync: Implement offline caching and periodic background synchronization. OCR Verification: Combine AI-based ID recognition with manual approval option. Vehicle Management: Real-time tracking of battery and vehicle status; predictive restocking via analytics. Staff Operations: Provide training and digital checklists to reduce human error. Cost: Set up cloud cost monitoring and optimization alerts. Contingency Plans Enable offline mode for station staff when Internet is unavailable. Activate backup servers in case of major downtime. Provide manual check-in/out workflow for rentals during system outages. Deploy mobile maintenance team to handle vehicle or battery issues at stations. Suspend or limit reservations dynamically if vehicle supply falls below safe threshold. 8. Expected Outcomes Technical Improvements: Real-time monitoring of all EV stations and rental status. Automated verification and e-contract signing replace manual paperwork. Centralized dashboard for admins to manage fleet, customers, and staff. System scalable to 20+ rental stations in the next deployment phase. Long-term Value Establishes a reliable EV mobility infrastructure for urban areas. Builds data foundation for future AI-powered demand forecasting. Enables integration with smart city and green transportation networks. Serves as a reusable platform for expanding to nationwide EV-sharing projects. Short to Medium-term Benefits Faster customer onboarding (from 15 mins ‚Üí \u0026lt;5 mins). Increased fleet utilization rate by 30% through data-driven scheduling. Improved accuracy of rental records and payment reconciliation. Enhanced user satisfaction via seamless booking and transparent billing. "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.3-s3-vpc/5.3.2-test-gwe/","title":"Test the Gateway Endpoint","tags":[],"description":"","content":"Create S3 bucket Navigate to S3 management console In the Bucket console, choose Create bucket In the Create bucket console Name the bucket: choose a name that hasn\u0026rsquo;t been given to any bucket globally (hint: lab number and your name) Leave other fields as they are (default) Scroll down and choose Create bucket Successfully create S3 bucket. Connect to EC2 with session manager For this workshop, you will use AWS Session Manager to access several EC2 instances. Session Manager is a fully managed AWS Systems Manager capability that allows you to manage your Amazon EC2 instances and on-premises virtual machines (VMs) through an interactive one-click browser-based shell. Session Manager provides secure and auditable instance management without the need to open inbound ports, maintain bastion hosts, or manage SSH keys.\nFirst cloud journey Lab for indepth understanding of Session manager.\nIn the AWS Management Console, start typing Systems Manager in the quick search box and press Enter: From the Systems Manager menu, find Node Management in the left menu and click Session Manager: Click Start Session, and select the EC2 instance named Test-Gateway-Endpoint. This EC2 instance is already running in \u0026ldquo;VPC Cloud\u0026rdquo; and will be used to test connectivity to Amazon S3 through the Gateway endpoint you just created (s3-gwe).\nSession Manager will open a new browser tab with a shell prompt: sh-4.2 $\nYou have successfully start a session - connect to the EC2 instance in VPC cloud. In the next step, we will create a S3 bucket and a file in it.\nCreate a file and upload to s3 bucket Change to the ssm-user\u0026rsquo;s home directory by typing cd ~ in the CLI Create a new file to use for testing with the command fallocate -l 1G testfile.xyz, which will create a file of 1GB size named \u0026ldquo;testfile.xyz\u0026rdquo;. Upload file to S3 bucket with command aws s3 cp testfile.xyz s3://your-bucket-name. Replace your-bucket-name with the name of S3 bucket that you created earlier. You have successfully uploaded the file to your S3 bucket. You can now terminate the session.\nCheck object in S3 bucket Navigate to S3 console. Click the name of your s3 bucket In the Bucket console, you will see the file you have uploaded to your S3 bucket Section summary Congratulation on completing access to S3 from VPC. In this section, you created a Gateway endpoint for Amazon S3, and used the AWS CLI to upload an object. The upload worked because the Gateway endpoint allowed communication to S3, without needing an Internet Gateway attached to \u0026ldquo;VPC Cloud\u0026rdquo;. This demonstrates the functionality of the Gateway endpoint as a secure path to S3 without traversing the Public Internet.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.2-week2/","title":"Week 2 Worklog","tags":[],"description":"","content":"Week 2 Objectives: Identity Security: Completely eliminate the use of the Root account for daily administration tasks; establish a standard User/Group structure. Network Architecture: Design and deploy a Custom VPC instead of using the Default VPC, ensuring deep understanding of IP planning. Traffic Control: Configure Route Tables to clearly distinguish between Public and Private Subnets. Compliance: Enable Multi-Factor Authentication (MFA) as a mandatory requirement for all Console access accounts. Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T2.1 2 IAM - Securing Root: - Log in as Root, enable MFA (Virtual Authenticator App) - Remove all Access Keys for Root (if any) 09/15/2025 09/15/2025 Completed https://cloudjourney.awsstudygroup.com/ T2.2 2 IAM - Admin Group Setup: - Create IAM Group CloudAdmins - Attach policy AdministratorAccess (AWS Managed Policy) 09/15/2025 09/15/2025 Completed https://cloudjourney.awsstudygroup.com/ T2.3 3 IAM - User Creation: - Create IAM User for yourself - Set Password Policy (complexity, rotation) - Add to group CloudAdmins 09/16/2025 09/16/2025 Completed https://cloudjourney.awsstudygroup.com/ T2.4 4 VPC - IP Planning: - Calculate CIDR for VPC (10.0.0.0/16) - Design Subnets to support up to 65,536 IP addresses 09/17/2025 09/17/2025 Completed https://cloudjourney.awsstudygroup.com/ T2.5 4 VPC - Deploy VPC: - Initialize VPC in Region ap-southeast-1 (Singapore) - Tag with Project=FCJ 09/17/2025 09/17/2025 Completed https://cloudjourney.awsstudygroup.com/ T2.6 5 VPC - Subnet Design: - Create 4 Subnets: 2 Public (10.0.1.0/24, 10.0.2.0/24) - 2 Private (10.0.3.0/24, 10.0.4.0/24) - Distribute evenly across 2 AZs 09/18/2025 09/18/2025 Completed https://cloudjourney.awsstudygroup.com/ T2.7 5 VPC - Internet Access: - Create and attach Internet Gateway (IGW) to VPC - Configure Route Table of Public Subnet to route 0.0.0.0/0 to IGW 09/18/2025 09/19/2025 Completed https://cloudjourney.awsstudygroup.com/ T2.8 6 Security - Firewall Basics: - Create Security Group Web-SG - Allow Inbound HTTP (80) from 0.0.0.0/0 - Allow SSH (22) from MyIP 09/19/2025 09/21/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 2 Achievements: Account Security:\nTransitioned to using IAM User with MFA for login Root account protected with strong password and separate physical/application MFA Applied the principle of \u0026ldquo;Least Privilege\u0026rdquo; Network Infrastructure:\nA fully functional custom VPC operational Verified DNS resolution capability (DNS Hostnames enabled) Established 2-Tier Network Architecture model Architecture:\nReady for deploying Web applications and Databases in upcoming weeks Clear understanding of the difference between Public and Private Subnets Solid grasp of CIDR and subnet mask concepts Lessons Learned:\nSecurity Group is an instance-level firewall (stateful) Network ACL is a subnet-level firewall (stateless) Understanding of AWS Shared Responsibility Model "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.3-s3-vpc/","title":"Access S3 from VPC","tags":[],"description":"","content":"Using Gateway endpoint In this section, you will create a Gateway eendpoint to access Amazon S3 from an EC2 instance. The Gateway endpoint will allow upload an object to S3 buckets without using the Public Internet. To create an endpoint, you must specify the VPC in which you want to create the endpoint, and the service (in this case, S3) to which you want to establish the connection.\nContent Create gateway endpoint Test gateway endpoint "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/4-eventparticipated/4.3-event3/","title":"Event 3","tags":[],"description":"","content":"EVENT HARVEST REPORT Event: AWS Cloud Mastery Series #2: From DevOps, IaC to Container \u0026amp; Observability\n1. ATTENDANCE OBJECTIVES Event participation focused on 4 core objectives to enhance technical capabilities and strategic thinking:\nShaping Mindset: Grasp the essence of the Value Cycle and DevOps mission in ensuring continuous and reliable software flow. Infrastructure Automation (IaC): End the era of manual operations (ClickOps), transition to infrastructure management through code with three key tools: CloudFormation, Terraform, and CDK. Containerization Strategy: Deep understanding of architecture and criteria for selecting optimal Container operating platforms: App Runner for simplicity, ECS for efficiency, or EKS for flexibility. Proactive Monitoring (Observability): Establish comprehensive \u0026ldquo;eyes and ears\u0026rdquo; system to detect incidents and optimize performance through CloudWatch and X-Ray. 2. SPEAKER LIST The event brought together top experts from AWS and Cloud Engineering:\nAWS Experts \u0026amp; Cloud Engineers Team ‚Äì Engineers and solution architects sharing Platform Engineering models and hands-on technical demos. 3. TECHNICAL CONTENT HIGHLIGHTS 3.1. DevOps Mindset \u0026amp; CI/CD Pipeline (Foundation of Thinking) The event redefined DevOps not just as tools, but as a culture optimizing value flow:\nThe Value Cycle: A complete 5-stage cycle from Insights $\\rightarrow$ Portfolio $\\rightarrow$ CI $\\rightarrow$ Testing $\\rightarrow$ CD. The goal is balancing Release Speed and Stability. Clearly distinguish automation levels: Continuous Integration (CI): Daily code merges, automatic Build and Test to detect errors early (Fail fast). Continuous Delivery: Automatic deployment to Staging, but requires human approval (Manual Trigger) for Production. Continuous Deployment: 100% automation of flow from Commit to Production. Optimal Pipeline Strategy: Centralized CI: Build centralized CI for management but empower Self-service for Developers. Artifact Management: Follow the principle \u0026ldquo;Build Once, Deploy Anywhere\u0026rdquo;. Package source code only once (Artifact) and use it for all environments to ensure consistency. Fail Conditions: Pipeline must stop immediately if encountering compilation errors, code standard violations, security vulnerabilities, or slow tests. Metrics: Use Heatmap and DORA metrics (Deploy frequency, Failure rate, MTTR) to measure process health. 3.2. Infrastructure as Code (IaC) - From ClickOps To Code Journey to eliminate risks from manual operations (ClickOps) towards Accuracy, Scalability, and Collaboration:\nAWS CloudFormation (Native): Use YAML/JSON to define resources. Manage lifecycle through Stack concept (deleting Stack cleans up resources). Terraform (Multi-Cloud): Open-source power with HCL language. Standard workflow: Write $\\rightarrow$ Plan (Preview changes) $\\rightarrow$ Apply. Strength is multi-platform management capability (AWS, Azure, GCP) and state management through State File. AWS CDK (Code-based): Define infrastructure using programming languages (Python, TypeScript\u0026hellip;). Constructs: From L1 (Detailed configuration) to L3 (Pre-built architecture Patterns). Drift Detection: Critical feature to detect configuration drift from manual edits, helping maintain system discipline. 3.3. Containerization - Application Running Strategy Analysis of Orchestration and Compute choices:\nComparing ECS vs. EKS: Amazon ECS: Simple, tightly integrated with AWS ecosystem, suitable for teams wanting fast deployment and reduced operations. Amazon EKS: Kubernetes standard, powerful and flexible, for complex Enterprise systems or Hybrid-cloud. Compute Models: EC2 Launch Type: Maximum control but requires server management effort. AWS Fargate: Serverless for containers, only need to care about CPU/RAM, AWS handles underlying infrastructure. AWS App Runner: \u0026ldquo;Zero-ops\u0026rdquo; solution, turn code/image into Web App with HTTPS in just a few steps, no network/server configuration needed. 3.4. Observability - Monitoring \u0026amp; Optimization Ensure deep and broad observability for stable operations:\nAmazon CloudWatch: Collect performance data (Metrics), centralized logs (Logs), and automatic response (Alarms) when system encounters issues. AWS X-Ray: Distributed Tracing tool, helps trace request journey through microservices to find Bottlenecks and root causes. Best Practices: Clearly distinguish roles of Logs (events) and Traces (journeys); use standard AWS Patterns to set up monitoring. 4. DETAILED EVENT EXPERIENCE This topic completely changed my perspective on system operations:\n4.1. Shift from \u0026ldquo;Ops\u0026rdquo; to \u0026ldquo;Platform Engineering\u0026rdquo; I realized modern DevOps is not about running after Developers to deploy for them. DevOps is about building \u0026ldquo;Highways\u0026rdquo; (Pipeline \u0026amp; Platform). A good platform is where Developers can self-serve infrastructure needs within the safe framework (Governance) that the DevOps team has established.\n4.2. Operational Discipline The principles of Artifact Management and Drift Detection are valuable lessons. In enterprise environments, Consistency is a critical factor. Absolutely do not rebuild code in different environments and strictly prohibit manual configuration changes when IaC has been applied.\n4.3. Smart Tool Selection Strategy There is no \u0026ldquo;best\u0026rdquo; tool, only the \u0026ldquo;most suitable\u0026rdquo; choice:\nNeed stability and native support: Choose CloudFormation. Multi-cloud enterprise: Terraform is mandatory. Team strong in programming, needs to reuse complex architecture: AWS CDK is the ultimate weapon. Simple Web App: Use App Runner instead of wasting resources operating Kubernetes. 5. CONCLUSION The \u0026ldquo;DevOps \u0026amp; IaC Mastery\u0026rdquo; topic painted a complete roadmap for Cloud modernization journey:\nAbout Mindset: Complete transition from manual to automation and data-driven measurement. About Infrastructure: Master IaC to ensure scalability, reproducibility, and configuration control. About Operations: The combination of flexible Containerization and deep Observability is the key to a high-performance and sustainable system. This is a solid knowledge foundation for me to confidently build and operate large-scale software systems on AWS.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.3-setup-bedrock/","title":"Setup AWS Bedrock","tags":[],"description":"","content":"Setting up AWS Bedrock \u0026amp; Knowledge Base In this section, you will configure AWS Bedrock to use Claude 3.5 Sonnet and create a Knowledge Base for document retrieval.\nStep 1: Enable Model Access IMPORTANT: You must enable model access before using Bedrock, otherwise you\u0026rsquo;ll get ValidationException errors.\nGo to AWS Console ‚Üí Services ‚Üí Bedrock In the left sidebar, click Model access (under Foundation models) Click Manage model access button (orange) Find and select these models: ‚úÖ Anthropic - Claude 3.5 Sonnet v2 (anthropic.claude-3-5-sonnet-20241022-v2:0) ‚úÖ Amazon - Titan Embeddings G1 - Text (for Knowledge Base) Click Request model access (bottom right) Wait for approval: Instant access models: Available immediately (green ‚úÖ) Other models: Wait 5-30 minutes (status changes from \u0026ldquo;In progress\u0026rdquo; ‚Üí \u0026ldquo;Access granted\u0026rdquo;) Verify models are enabled:\n# Using AWS CLI aws bedrock list-foundation-models --region us-west-2 # Or check in Console: # Bedrock ‚Üí Model access ‚Üí Status must be \u0026#34;Access granted\u0026#34; Step 2: Create S3 Bucket for Knowledge Base Knowledge Base requires an S3 bucket to store documents.\nGo to S3 ‚Üí Create bucket Bucket name: ev-rental-knowledge-docs (must be globally unique) Region: Same as your Bedrock region (e.g., us-west-2) Block all public access: ‚úÖ Enabled (recommended) Click Create bucket Step 3: Upload Documents to S3 Upload your rental policy documents (PDF, TXT, DOCX):\nSample documents to upload:\nrental-policy.pdf - Rental policies and terms pricing.pdf - Vehicle pricing information faq.txt - Frequently asked questions booking-process.pdf - How to book a vehicle Upload via Console:\nGo to your S3 bucket: ev-rental-knowledge-docs Click Upload ‚Üí Add files Select your documents Click Upload Upload via AWS CLI:\naws s3 cp rental-policy.pdf s3://ev-rental-knowledge-docs/ aws s3 cp pricing.pdf s3://ev-rental-knowledge-docs/ aws s3 cp faq.txt s3://ev-rental-knowledge-docs/ aws s3 cp booking-process.pdf s3://ev-rental-knowledge-docs/ Step 4: Create Knowledge Base Go to Bedrock ‚Üí Knowledge Bases ‚Üí Create Knowledge base name: ev-rental-knowledge-base Description: \u0026ldquo;VinFast EV rental policies and FAQs\u0026rdquo; Click Next Data source configuration:\nData source name: rental-docs S3 URI: s3://ev-rental-knowledge-docs/ Click Next Embeddings model:\nSelect: Titan Embeddings G1 - Text (amazon.titan-embed-text-v1) Vector database: Choose Bedrock managed (OpenSearch Serverless) (easiest option) Click Next Review and create:\nReview all settings Click Create knowledge base Wait for creation to complete (2-3 minutes) Step 5: Sync Data Source After Knowledge Base is created, you need to sync the data:\nIn your Knowledge Base, go to Data sources tab Select your data source: rental-docs Click Sync button Wait for sync to complete (check status: \u0026ldquo;Syncing\u0026rdquo; ‚Üí \u0026ldquo;Ready\u0026rdquo;) This process indexes all documents and creates vector embeddings Sync status:\nüîÑ Syncing: In progress ‚úÖ Ready: Completed successfully ‚ùå Failed: Check S3 permissions or document formats Step 6: Get Knowledge Base ID You\u0026rsquo;ll need this ID for your backend application:\nIn your Knowledge Base page Copy the Knowledge Base ID (format: 89CI1JSSE4 or similar) Save it in your notes - you\u0026rsquo;ll use it in the next step Example Knowledge Base ID:\nKnowledge Base ID: 89CI1JSSE4 Knowledge Base ARN: arn:aws:bedrock:us-west-2:123456789:knowledge-base/89CI1JSSE4 Step 7: Test Knowledge Base (Optional) Test your Knowledge Base directly in the console:\nGo to your Knowledge Base Click Test tab Enter a question: \u0026ldquo;What is the rental policy?\u0026rdquo; Click Run Verify it returns relevant information from your documents Verification Checklist Before moving to the next step, ensure:\n‚úÖ Claude 3.5 Sonnet v2 model access is granted ‚úÖ Titan Embeddings model access is granted ‚úÖ S3 bucket created with documents uploaded ‚úÖ Knowledge Base created and data synced successfully ‚úÖ Knowledge Base ID saved ‚úÖ Test query returns relevant results Troubleshooting Issue: \u0026ldquo;ValidationException: Model not enabled\u0026rdquo;\nSolution: Go to Bedrock ‚Üí Model access and enable the model Issue: \u0026ldquo;Sync failed\u0026rdquo;\nCheck S3 bucket permissions Verify document formats (PDF, TXT, DOCX supported) Check CloudWatch Logs for detailed errors Issue: \u0026ldquo;No results from Knowledge Base\u0026rdquo;\nEnsure documents are uploaded to S3 Run sync again Wait a few minutes after sync completes Try different query phrasing Next: Proceed to Deploy Backend API to build the FastAPI server.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.4-s3-onprem/5.4.3-test-endpoint/","title":"Test the Interface Endpoint","tags":[],"description":"","content":"Get the regional DNS name of S3 interface endpoint From the Amazon VPC menu, choose Endpoints.\nClick the name of newly created endpoint: s3-interface-endpoint. Click details and save the regional DNS name of the endpoint (the first one) to your text-editor for later use.\nConnect to EC2 instance in \u0026ldquo;VPC On-prem\u0026rdquo; Navigate to Session manager by typing \u0026ldquo;session manager\u0026rdquo; in the search box\nClick Start Session, and select the EC2 instance named Test-Interface-Endpoint. This EC2 instance is running in \u0026ldquo;VPC On-prem\u0026rdquo; and will be used to test connectivty to Amazon S3 through the Interface endpoint we just created. Session Manager will open a new browser tab with a shell prompt: sh-4.2 $\nChange to the ssm-user\u0026rsquo;s home directory with command \u0026ldquo;cd ~\u0026rdquo;\nCreate a file named testfile2.xyz\nfallocate -l 1G testfile2.xyz Copy file to the same S3 bucket we created in section 3.2 aws s3 cp --endpoint-url https://bucket.\u0026lt;Regional-DNS-Name\u0026gt; testfile2.xyz s3://\u0026lt;your-bucket-name\u0026gt; This command requires the \u0026ndash;endpoint-url parameter, because you need to use the endpoint-specific DNS name to access S3 using an Interface endpoint. Do not include the leading \u0026rsquo; * \u0026rsquo; when copying/pasting the regional DNS name. Provide your S3 bucket name created earlier Now the file has been added to your S3 bucket. Let check your S3 bucket in the next step.\nCheck Object in S3 bucket Navigate to S3 console Click Buckets Click the name of your bucket and you will see testfile2.xyz has been added to your bucket "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/3-blogstranslated/","title":"Translated Blogs","tags":[],"description":"","content":" ‚ö†Ô∏è Note: The information below is for reference purposes only. Please do not copy verbatim for your own report, including this warning.\nThis section will list and introduce the blogs you have translated. For example:\nBlog 1 - Getting started with healthcare data lakes: Using microservices This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 2 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 3 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 4 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 5 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\nBlog 6 - \u0026hellip; This blog introduces how to start building a data lake in the healthcare sector by applying a microservices architecture. You will learn why data lakes are important for storing and analyzing diverse healthcare data (electronic medical records, lab test data, medical IoT devices‚Ä¶), how microservices help make the system more flexible, scalable, and easier to maintain. The article also guides you through the steps to set up the environment, organize the data processing pipeline, and ensure compliance with security \u0026amp; privacy standards such as HIPAA.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.3-week3/","title":"Week 3 Worklog","tags":[],"description":"","content":"Week 3 Objectives: Compute Deployment: Successfully launch an EC2 Instance in the Public Subnet of the created VPC. Access Management: Configure Key Pair (ED25519) and Security Group for secure SSH access. Application Authorization: Use IAM Role to grant S3 access to EC2 without storing Access Keys on the machine. Block Storage: Create, attach, and format an additional EBS Volume to understand persistent storage. Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T3.1 2 EC2 - AMI Selection: - Select Amazon Linux 2023 AMI (HVM) - Optimize default performance and security 09/22/2025 09/22/2025 Completed https://cloudjourney.awsstudygroup.com/ T3.2 2 Security - Key Management: - Create ED25519 Key Pair (more secure than RSA) - Store .pem file locally with permission 400 09/22/2025 09/22/2025 Completed https://cloudjourney.awsstudygroup.com/ T3.3 3 Compute - Launch Instance: - Launch t3.micro instance (Free Tier) - In Public Subnet 1 - Assign Security Group Web-SG created in Week 2 09/23/2025 09/23/2025 Completed https://cloudjourney.awsstudygroup.com/ T3.4 4 IAM - Role Creation: - Create IAM Role EC2-S3-Access-Role - Policy: AmazonS3ReadOnlyAccess - Trust entity: ec2.amazonaws.com 09/24/2025 09/24/2025 Completed https://cloudjourney.awsstudygroup.com/ T3.5 4 Compute - Attach Role: - Attach IAM Role to running instance - Via Actions \u0026gt; Security \u0026gt; Modify IAM Role 09/24/2025 09/25/2025 Completed https://cloudjourney.awsstudygroup.com/ T3.6 5 CLI - Verification: - SSH into instance - Install AWS CLI (if not available) - Run aws s3 ls to verify access 09/25/2025 09/26/2025 Completed https://cloudjourney.awsstudygroup.com/ T3.7 6 Storage - EBS Operations: - Create 1GB gp3 EBS volume in same AZ as instance - Attach to instance - Use lsblk, mkfs -t xfs, and mount commands 09/26/2025 09/28/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 3 Achievements: Operational Infrastructure:\nFirst Web server is online with Public IP Accessible via secure SSH Clear understanding of differences between Instance Types (T3, C5, R5) Application Security:\nDemonstrated EC2 can access S3 Buckets without aws configure No need to store Access Keys on server Applied \u0026ldquo;Temporary Credentials\u0026rdquo; mechanism through IAM Role Storage:\nUnderstanding difference between EBS (persistent) and Instance Store (ephemeral) Practiced attaching and mounting EBS volume Know how to format and use new disk Troubleshooting:\nInitially encountered \u0026ldquo;Connection Timeout\u0026rdquo; error when SSH Cause: Forgot to add Inbound Port 22 rule in Security Group Fixed and learned troubleshooting experience Skills:\nProficient in launching and managing EC2 instances Understanding of instance lifecycle (Launch, Stop, Start, Terminate) Solid grasp of Instance Profiles and IAM Roles for EC2 concepts "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.4-s3-onprem/","title":"Access S3 from on-premises","tags":[],"description":"","content":"Overview In this section, you will create an Interface endpoint to access Amazon S3 from a simulated on-premises environment. The Interface endpoint will allow you to route to Amazon S3 over a VPN connection from your simulated on-premises environment.\nWhy using Interface endpoint:\nGateway endpoints only work with resources running in the VPC where they are created. Interface endpoints work with resources running in VPC, and also resources running in on-premises environments. Connectivty from your on-premises environment to the cloud can be provided by AWS Site-to-Site VPN or AWS Direct Connect. Interface endpoints allow you to connect to services powered by AWS PrivateLink. These services include some AWS services, services hosted by other AWS customers and partners in their own VPCs (referred to as PrivateLink Endpoint Services), and supported AWS Marketplace Partner services. For this workshop, we will focus on connecting to Amazon S3. "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.4-deploy-backend/","title":"Deploy Backend API","tags":[],"description":"","content":"Deploy Backend API with FastAPI In this section, you will set up the FastAPI backend server that orchestrates the AI agent using Strands SDK.\nStep 1: Clone or Create Project Structure Create a new directory for the backend:\nmkdir ev-rental-backend cd ev-rental-backend Project structure:\nev-rental-backend/ ‚îú‚îÄ‚îÄ app/ ‚îÇ ‚îú‚îÄ‚îÄ __init__.py ‚îÇ ‚îú‚îÄ‚îÄ main.py # FastAPI app ‚îÇ ‚îú‚îÄ‚îÄ agent.py # Strands Agent setup ‚îÇ ‚îú‚îÄ‚îÄ tools.py # Agent tools (search vehicles, stations) ‚îÇ ‚îî‚îÄ‚îÄ database.py # PostgreSQL connection ‚îú‚îÄ‚îÄ requirements.txt # Python dependencies ‚îú‚îÄ‚îÄ .env # Environment variables ‚îî‚îÄ‚îÄ README.md Step 2: Install Dependencies Create requirements.txt:\nfastapi==0.104.1 uvicorn[standard]==0.24.0 strands-agent-sdk==0.1.5 boto3==1.34.10 psycopg2-binary==2.9.9 sqlalchemy==2.0.23 pydantic==2.5.2 python-dotenv==1.0.0 httpx==0.25.2 Install dependencies:\n# Create virtual environment python -m venv venv # Activate virtual environment # Windows: venv\\Scripts\\activate # macOS/Linux: source venv/bin/activate # Install packages pip install -r requirements.txt Step 3: Configure Environment Variables Create .env file with your AWS credentials and Knowledge Base ID:\n# AWS Credentials AWS_ACCESS_KEY_ID=YOUR_ACCESS_KEY_ID AWS_SECRET_ACCESS_KEY=YOUR_SECRET_ACCESS_KEY AWS_REGION=us-west-2 # Bedrock Configuration BEDROCK_MODEL_ID=anthropic.claude-3-5-sonnet-20241022-v2:0 KNOWLEDGE_BASE_ID=89CI1JSSE4 # Database Configuration DATABASE_URL=postgresql://postgres:password@localhost:5432/ev_rental_db # API Configuration BACKEND_API_URL=http://localhost:8080 ‚ö†Ô∏è Security Note:\nNever commit .env to Git Add .env to .gitignore Step 4: Create Database Models Create app/database.py:\nfrom sqlalchemy import create_engine, Column, Integer, String, Text, DateTime from sqlalchemy.ext.declarative import declarative_base from sqlalchemy.orm import sessionmaker from datetime import datetime import os DATABASE_URL = os.getenv(\u0026#34;DATABASE_URL\u0026#34;) engine = create_engine(DATABASE_URL) SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine) Base = declarative_base() class ChatHistory(Base): __tablename__ = \u0026#34;chat_history\u0026#34; id = Column(Integer, primary_key=True, index=True) session_id = Column(String, index=True) user_message = Column(Text) agent_response = Column(Text) timestamp = Column(DateTime, default=datetime.utcnow) # Create tables Base.metadata.create_all(bind=engine) Step 5: Create Agent Tools Create app/tools.py:\nimport httpx import os from typing import List, Dict BACKEND_API_URL = os.getenv(\u0026#34;BACKEND_API_URL\u0026#34;, \u0026#34;http://localhost:8080\u0026#34;) async def search_vehicles(location: str = None, model: str = None) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Search for available vehicles\u0026#34;\u0026#34;\u0026#34; async with httpx.AsyncClient() as client: params = {} if location: params[\u0026#34;location\u0026#34;] = location if model: params[\u0026#34;model\u0026#34;] = model response = await client.get(f\u0026#34;{BACKEND_API_URL}/api/vehicles\u0026#34;, params=params) return response.json() async def search_stations(city: str = None) -\u0026gt; List[Dict]: \u0026#34;\u0026#34;\u0026#34;Search for charging stations\u0026#34;\u0026#34;\u0026#34; async with httpx.AsyncClient() as client: params = {} if city: params[\u0026#34;city\u0026#34;] = city response = await client.get(f\u0026#34;{BACKEND_API_URL}/api/stations\u0026#34;, params=params) return response.json() Step 6: Setup Strands Agent Create app/agent.py:\nimport boto3 import os from strands_agent import Agent, Tool # Initialize Bedrock client bedrock_client = boto3.client( \u0026#39;bedrock-runtime\u0026#39;, region_name=os.getenv(\u0026#39;AWS_REGION\u0026#39;), aws_access_key_id=os.getenv(\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;), aws_secret_access_key=os.getenv(\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;) ) # Initialize Knowledge Base client bedrock_agent_client = boto3.client( \u0026#39;bedrock-agent-runtime\u0026#39;, region_name=os.getenv(\u0026#39;AWS_REGION\u0026#39;), aws_access_key_id=os.getenv(\u0026#39;AWS_ACCESS_KEY_ID\u0026#39;), aws_secret_access_key=os.getenv(\u0026#39;AWS_SECRET_ACCESS_KEY\u0026#39;) ) # Create Agent agent = Agent( model_id=os.getenv(\u0026#39;BEDROCK_MODEL_ID\u0026#39;), client=bedrock_client, knowledge_base_id=os.getenv(\u0026#39;KNOWLEDGE_BASE_ID\u0026#39;), tools=[ Tool( name=\u0026#34;search_vehicles\u0026#34;, description=\u0026#34;Search for available electric vehicles for rent\u0026#34;, function=search_vehicles ), Tool( name=\u0026#34;search_stations\u0026#34;, description=\u0026#34;Find nearby charging stations\u0026#34;, function=search_stations ) ] ) Step 7: Create FastAPI Application Create app/main.py:\nfrom fastapi import FastAPI, HTTPException from fastapi.middleware.cors import CORSMiddleware from pydantic import BaseModel from app.agent import agent from app.database import SessionLocal, ChatHistory import uuid app = FastAPI(title=\u0026#34;EV Rental AI Agent API\u0026#34;) # Enable CORS app.add_middleware( CORSMiddleware, allow_origins=[\u0026#34;*\u0026#34;], allow_credentials=True, allow_methods=[\u0026#34;*\u0026#34;], allow_headers=[\u0026#34;*\u0026#34;], ) class ChatRequest(BaseModel): message: str session_id: str = None class ChatResponse(BaseModel): response: str session_id: str data: dict = None @app.post(\u0026#34;/chat\u0026#34;, response_model=ChatResponse) async def chat(request: ChatRequest): try: # Generate session ID if not provided session_id = request.session_id or str(uuid.uuid4()) # Get agent response agent_response = await agent.run(request.message) # Save to database db = SessionLocal() chat_record = ChatHistory( session_id=session_id, user_message=request.message, agent_response=agent_response[\u0026#34;response\u0026#34;] ) db.add(chat_record) db.commit() db.close() return ChatResponse( response=agent_response[\u0026#34;response\u0026#34;], session_id=session_id, data=agent_response.get(\u0026#34;data\u0026#34;) ) except Exception as e: raise HTTPException(status_code=500, detail=str(e)) @app.get(\u0026#34;/health\u0026#34;) async def health_check(): return {\u0026#34;status\u0026#34;: \u0026#34;healthy\u0026#34;} Step 8: Run the Backend Server Start the FastAPI server:\n# Make sure virtual environment is activated uvicorn app.main:app --reload --port 8000 # You should see: # INFO: Uvicorn running on http://127.0.0.1:8000 # INFO: Application startup complete. Step 9: Test the API Test health endpoint:\ncurl http://localhost:8000/health # Response: {\u0026#34;status\u0026#34;:\u0026#34;healthy\u0026#34;} Test chat endpoint:\ncurl -X POST http://localhost:8000/chat \\ -H \u0026#34;Content-Type: application/json\u0026#34; \\ -d \u0026#39;{\u0026#34;message\u0026#34;: \u0026#34;Ch√≠nh s√°ch thu√™ xe c·ªßa b·∫°n l√† g√¨?\u0026#34;}\u0026#39; Expected response:\n{ \u0026#34;response\u0026#34;: \u0026#34;## üìã Ch√≠nh s√°ch thu√™ xe VinFast\\n\\n### üìÑ Gi·∫•y t·ªù c·∫ßn thi·∫øt:\\n- CMND/CCCD...\u0026#34;, \u0026#34;session_id\u0026#34;: \u0026#34;abc123-...\u0026#34;, \u0026#34;data\u0026#34;: null } Verification Checklist Before proceeding, ensure:\n‚úÖ Virtual environment created and activated ‚úÖ All dependencies installed ‚úÖ .env file configured with AWS credentials ‚úÖ PostgreSQL database running and connected ‚úÖ FastAPI server running on port 8000 ‚úÖ Health check endpoint returns {\u0026quot;status\u0026quot;:\u0026quot;healthy\u0026quot;} ‚úÖ Chat endpoint returns proper responses Troubleshooting Issue: \u0026ldquo;ModuleNotFoundError\u0026rdquo;\nSolution: Ensure virtual environment is activated and dependencies installed Issue: \u0026ldquo;Database connection failed\u0026rdquo;\nCheck PostgreSQL is running Verify DATABASE_URL in .env Test connection: psql -h localhost -U postgres -d ev_rental_db Issue: \u0026ldquo;Bedrock ValidationException\u0026rdquo;\nVerify AWS credentials in .env Ensure model access is granted in Bedrock console Check KNOWLEDGE_BASE_ID is correct Next: Proceed to Deploy Frontend to create the React chat interface.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/4-eventparticipated/4.4-event4/","title":"Event 4","tags":[],"description":"","content":"EVENT HARVEST REPORT Event: AWS Cloud Mastery Series #3: Cloud Security \u0026amp; Operations\n1. ATTENDANCE OBJECTIVES This event series doesn\u0026rsquo;t just introduce tools, but aims to build solid System Thinking to transition from traditional infrastructure to Cloud-Native Security model. Core objectives include:\nCommunity Connection: Spread the learning spirit through AWS Cloud Clubs. Establishing Governance Foundation: Ensure compliance when operating large-scale systems with hundreds of AWS accounts. Defense in Depth: Eliminate single points of failure by closely coordinating Identity, Network, and Data Protection. Automated Response: Address human latency with automated incident handling processes. 2. SPEAKER LIST The event brought together experts from AWS Community, Cloud Engineers, and core members of the First Cloud Journey program:\nAWS Cloud Clubs Captains: Le Vu Xuan An (HCMUTE), Tran Duc Anh (SGU), Tran Doan Cong Ly (PTIT), Danh Hoang Hieu Nghi (HUFLIT) Identity \u0026amp; Governance: Huynh Hoang Long, Dinh Le Hoang Anh (AWS Community Builders) Detection \u0026amp; Monitoring: Tran Duc Anh, Nguyen Tuan Thinh, Nguyen Do Thanh Dat Network Security: Kha Van (Cloud Security Engineer | AWS Community Builder) Data Protection: Thinh Lam, Viet Nguyen Incident Response: Mendel Grabski (Long) - ex Head of Security \u0026amp; DevOps, Tinh Truong - Platform Engineer 3. TECHNICAL CONTENT HIGHLIGHTS 3.1. AWS Cloud Clubs \u0026amp; Development Opportunities Opening with introduction to AWS Cloud Clubs - the incubator for future Cloud talent:\nVision: Empower students to master cloud technology, develop leadership skills, and connect globally. Benefits: \u0026ldquo;Learning by doing\u0026rdquo; through real projects, receiving AWS exam vouchers, Udemy accounts, and career opportunities. The Badging Journey: Gamified advancement path through levels from Bronze to Diamond. Attractive rewards include AWS Credits ($200+), exam vouchers, and privileges at Student Community Day. 3.2. Identity And Governance Foundation Controlling \u0026ldquo;Who can do what\u0026rdquo; is the first step of security:\nModern IAM Mindset: Identity is the new firewall on Cloud. Absolute priority for Short-term Credentials (self-expiring STS tokens) instead of Long-term Credentials (risky Access Keys). Least Privilege Principle: Minimize use of wildcard * in Policies. Large-scale Governance: Use AWS Organizations to divide organization into isolated units (OUs). Apply Service Control Policies (SCPs) as a \u0026ldquo;constitution\u0026rdquo; to establish inviolable Guardrails, blocking dangerous actions at the root. 3.3. Visibility And Detection Capability Cannot protect what we cannot see:\nAmazon GuardDuty: \u0026ldquo;Scout\u0026rdquo; using Machine Learning to examine 3 data sources: CloudTrail, VPC Flow Logs, and DNS Logs. Runtime Monitoring feature allows deep OS visibility to catch unusual processes or privilege escalation behavior. AWS Security Hub: \u0026ldquo;Command center\u0026rdquo; helping standardize all alerts from multiple sources to common ASFF format. Simultaneously auto-assesses system compliance against international security standards (CIS, PCI-DSS). 3.4. Network Security Strategy for building multi-layered \u0026ldquo;Digital Fortress\u0026rdquo;:\nBasic Controls: Use Security Groups (Stateful) with Micro-segmentation technique (reference by group instead of hard IP). Combine NACLs (Stateless) for coarse blocking at Subnet boundaries. Advanced Defense: DNS Firewall: Block connections to hacker C2 servers right at domain resolution stage. AWS Network Firewall: Next-generation firewall with Deep Packet Inspection (DPI), combined with Stateful engine filters (Suricata compatible) to control Internet-bound traffic. Modern Architecture: Integrate AWS Transit Gateway to simplify routing and apply Active Threat Defense - automatically sync blacklists from GuardDuty to block threats instantly. 3.5. Data Protection Protect digital assets with intelligent encryption:\nEnvelope Encryption: Understand KMS mechanism: Master Key protects Data Key, and Data Key directly encrypts data. This ensures maximum performance and security. Secret Management: Instead of hardcoding passwords, use AWS Secrets Manager combined with Lambda for periodic Database password Automatic Rotation. Hardware Infrastructure: Leverage AWS Nitro System to push encryption tasks to dedicated chips, ensuring data protection without impacting server CPU performance. 3.6. Incident Response Response process determines damage level when defense fails:\nPrevention Strategy: \u0026ldquo;Prevention is better than cure\u0026rdquo; by eliminating SSH/long-term Keys, blocking Public S3. Mandate infrastructure management via Code (IaC) to avoid manual errors (ClickOps). 5-Step Process: Prepare $\\rightarrow$ Detect $\\rightarrow$ Isolate (change Security Group/revoke IAM permissions) $\\rightarrow$ Eradicate \u0026amp; Recover $\\rightarrow$ Post-incident (Lessons learned). Automation is key: Humans cannot be faster than machines. Need to use EventBridge + Lambda to automatically isolate infected resources or fix configuration errors in just seconds. 4. DETAILED EVENT EXPERIENCE This topic series completely changed how I view Cloud security:\n4.1. \u0026ldquo;Security by Design\u0026rdquo; Mindset The most important highlight is shifting from \u0026ldquo;Security is a wall\u0026rdquo; thinking to \u0026ldquo;Security is DNA\u0026rdquo;. Every service, every architecture must be designed with security from the start, not as a final patch layer.\n4.2. Power of Automation The automated Incident Response demo impressed me deeply. Using EventBridge combined with Lambda to react to abnormal events in seconds instead of waiting for human intervention is truly a major advancement in security operations.\n4.3. Community is the Key AWS Cloud Clubs is not just a place to learn technical skills but also an environment to develop soft skills, networking, and career opportunities. The Badging Journey has gamified the learning process, creating strong motivation for self-development.\n5. CONCLUSION The \u0026ldquo;Cloud Security \u0026amp; Operations Mastery\u0026rdquo; topic series painted a comprehensive roadmap to build secure systems on AWS:\nGovernance \u0026amp; Identity: Build solid foundation from organizational policies and user management. Network \u0026amp; Monitoring: Establish multi-layered defense system with deep and broad observability. Data \u0026amp; Response: Protect core assets with encryption and prepare automated response scenarios to ensure business continuity. This is important foundational knowledge for me to confidently deploy and operate Cloud systems with comprehensive security mindset.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/4-eventparticipated/","title":"Events Participated","tags":[],"description":"","content":"Throughout my internship at First Cloud Journey, I had the opportunity to participate in 4 significant technology events. Each event not only provided in-depth knowledge about AWS Cloud and GenAI, but also served as an opportunity to expand my professional network, learn from industry-leading experts, and experience memorable moments with the Cloud Builders community.\nEvent 1 Event Name: Vietnam Cloud Day 2025 - Ho Chi Minh City Connect Edition for Builders (Track 1: GenAI \u0026amp; Data)\nDate \u0026amp; Time: 09:00, September 18, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 2 Event Name: AWS Cloud Mastery Series #1\nDate \u0026amp; Time: 08:30, November 15, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 3 Event Name: AWS Cloud Mastery Series #2\nDate \u0026amp; Time: 08:30, November 17, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\nEvent 4 Event Name: AWS Cloud Mastery Series #3\nDate \u0026amp; Time: 08:30, November 29, 2025\nLocation: 26th Floor, Bitexco Tower, 02 Hai Trieu Street, Saigon Ward, Ho Chi Minh City\nRole: Attendee\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.4-s3-onprem/5.4.4-dns-simulation/","title":"On-premises DNS Simulation","tags":[],"description":"","content":"AWS PrivateLink endpoints have a fixed IP address in each Availability Zone where they are deployed, for the life of the endpoint (until it is deleted). These IP addresses are attached to Elastic Network Interfaces (ENIs). AWS recommends using DNS to resolve the IP addresses for endpoints so that downstream applications use the latest IP addresses when ENIs are added to new AZs, or deleted over time.\nIn this section, you will create a forwarding rule to send DNS resolution requests from a simulated on-premises environment to a Route 53 Private Hosted Zone. This section leverages the infrastructure deployed by CloudFormation in the Prepare the environment section.\nCreate DNS Alias Records for the Interface endpoint Navigate to the Route 53 management console (Hosted Zones section). The CloudFormation template you deployed in the Prepare the environment section created this Private Hosted Zone. Click on the name of the Private Hosted Zone, s3.us-east-1.amazonaws.com: Create a new record in the Private Hosted Zone: Record name and record type keep default options Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor (you saved when doing section 4.3) Click Add another record, and add a second record using the following values. Click Create records when finished to create both records. Record name: *. Record type: keep default value (type A) Alias Button: Click to enable Route traffic to: Alias to VPC Endpoint Region: US East (N. Virginia) [us-east-1] Choose endpoint: Paste the Regional VPC Endpoint DNS name from your text editor The new records appear in the Route 53 console:\nCreate a Resolver Forwarding Rule Route 53 Resolver Forwarding Rules allow you to forward DNS queries from your VPC to other sources for name resolution. Outside of a workshop environment, you might use this feature to forward DNS queries from your VPC to DNS servers running on-premises. In this section, you will simulate an on-premises conditional forwarder by creating a forwarding rule that forwards DNS queries for Amazon S3 to a Private Hosted Zone running in \u0026ldquo;VPC Cloud\u0026rdquo; in-order to resolve the PrivateLink interface endpoint regional DNS name.\nFrom the Route 53 management console, click Inbound endpoints on the left side bar In the Inbound endpoints console, click the ID of the inbound endpoint Copy the two IP addresses listed to your text editor From the Route 53 menu, choose Resolver \u0026gt; Rules, and click Create rule: In the Create rule console: Name: myS3Rule Rule type: Forward Domain name: s3.us-east-1.amazonaws.com VPC: VPC On-prem Outbound endpoint: VPCOnpremOutboundEndpoint Target IP Addresses: Enter both IP addresses from your text editor (inbound endpoint addresses) and then click Submit You have successfully created resolver forwarding rule.\nTest the on-premises DNS Simulation Connect to Test-Interface-Endpoint EC2 instance with Session manager Test DNS resolution. The dig command will return the IP addresses assigned to the VPC Interface endpoint running in VPC Cloud (your IP\u0026rsquo;s will be different): dig +short s3.us-east-1.amazonaws.com The IP addresses returned are the VPC endpoint IP addresses, NOT the Resolver IP addresses you pasted from your text editor. The IP addresses of the Resolver endpoint and the VPC endpoint look similar because they are all from the VPC Cloud CIDR block.\nNavigate to the VPC menu (Endpoints section), select the S3 Interface endpoint. Click the Subnets tab and verify that the IP addresses returned by Dig match the VPC endpoint: Return to your shell and use the AWS CLI to test listing your S3 buckets: aws s3 ls --endpoint-url https://s3.us-east-1.amazonaws.com Terminate your Session Manager session: In this section you created an Interface endpoint for Amazon S3. This endpoint can be reached from on-premises through Site-to-Site VPN or AWS Direct Connect. Route 53 Resolver outbound endpoints simulated forwarding DNS requests from on-premises to a Private Hosted Zone running the cloud. Route 53 inbound Endpoints recieved the resolution request and returned a response containing the IP addresses of the VPC interface endpoint. Using DNS to resolve the endpoint IP addresses provides high availability in-case of an Availability Zone outage.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.4-week4/","title":"Week 4 Worklog","tags":[],"description":"","content":"Week 4 Objectives: Dev Environment: Set up AWS Cloud9 as a unified development environment. Serverless Web: Deploy a static website on S3, configure Bucket Policy to allow safe public access. Database: Initialize Amazon RDS MySQL in Private Subnet to ensure security. Multi-Tier Connectivity: Establish connection from EC2/Cloud9 (Public Subnet) to RDS (Private Subnet). Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T4.1 2 Cloud9 - Setup IDE: - Initialize Cloud9 environment (EC2 t3.small) in VPC - Enable Auto-hibernate after 30 minutes 09/29/2025 09/29/2025 Completed https://cloudjourney.awsstudygroup.com/ T4.2 3 S3 - Static Hosting: - Create S3 Bucket with unique name - Enable \u0026ldquo;Static website hosting\u0026rdquo; - Upload index.html and error.html 09/30/2025 09/30/2025 Completed https://cloudjourney.awsstudygroup.com/ T4.3 3 S3 - Public Policy: - Disable \u0026ldquo;Block Public Access\u0026rdquo; - Write Bucket Policy (JSON) allowing s3:GetObject 09/30/2025 10/01/2025 Completed https://cloudjourney.awsstudygroup.com/ T4.4 4 RDS - Subnet Group: - Create DB Subnet Group - Include 2 Private Subnets created in Week 2 10/01/2025 10/01/2025 Completed https://cloudjourney.awsstudygroup.com/ T4.5 4 RDS - Launch DB: - Launch RDS MySQL (Free Tier) - Disable Multi-AZ - Disable Public Accessibility 10/01/2025 10/02/2025 Completed https://cloudjourney.awsstudygroup.com/ T4.6 5 Security - Security Chaining: - Configure RDS Security Group - Only allow Inbound Port 3306 from Cloud9/EC2 SG 10/02/2025 10/03/2025 Completed https://cloudjourney.awsstudygroup.com/ T4.7 6 Database - Connection Test: - Use terminal on Cloud9 - Connect MySQL: mysql -h \u0026lt;endpoint\u0026gt; -u admin -p 10/03/2025 10/05/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 4 Achievements: Web:\nStatic website operational at S3 Endpoint URL Clear understanding of how S3 can host websites without servers Solid grasp of writing Bucket Policy for safe public access Database:\nDB instance operating isolated in internal network Cannot access DB directly from Internet (correct security standard) Understanding of RDS Managed Service and its benefits Skills:\nMastered JSON syntax for S3 Policy Understanding of \u0026ldquo;Security Group Referencing\u0026rdquo; (nested SG references) Critical technique for building dynamic N-tier architecture Architecture:\nCompleted preliminary \u0026ldquo;3-Tier Web Architecture\u0026rdquo; model: Presentation Tier (S3) Application Tier (Cloud9/EC2) Data Tier (RDS) Understanding of separation of concerns in cloud architecture "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.5-deploy-frontend/","title":"Deploy Frontend","tags":[],"description":"","content":"Deploying the React Frontend In this section, you will set up and run the React chat interface that connects to your FastAPI backend.\nStep 1: Clone or Create React Project Create a new React application:\n# Using Create React App npx create-react-app ev-rental-frontend cd ev-rental-frontend # Or clone existing repository git clone https://github.com/your-org/ev-rental-frontend.git cd ev-rental-frontend Step 2: Install Dependencies Install required npm packages:\n# Core dependencies npm install axios react-markdown npm install @chakra-ui/react @emotion/react @emotion/styled framer-motion npm install react-icons # Or use package.json npm install Sample package.json dependencies:\n{ \u0026#34;dependencies\u0026#34;: { \u0026#34;react\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;react-dom\u0026#34;: \u0026#34;^18.2.0\u0026#34;, \u0026#34;axios\u0026#34;: \u0026#34;^1.6.2\u0026#34;, \u0026#34;@chakra-ui/react\u0026#34;: \u0026#34;^2.8.2\u0026#34;, \u0026#34;@emotion/react\u0026#34;: \u0026#34;^11.11.1\u0026#34;, \u0026#34;@emotion/styled\u0026#34;: \u0026#34;^11.11.0\u0026#34;, \u0026#34;framer-motion\u0026#34;: \u0026#34;^10.16.16\u0026#34;, \u0026#34;react-markdown\u0026#34;: \u0026#34;^9.0.1\u0026#34;, \u0026#34;react-icons\u0026#34;: \u0026#34;^4.12.0\u0026#34; } } Step 3: Project Structure Your frontend should have this structure:\nev-rental-frontend/ ‚îú‚îÄ‚îÄ public/ ‚îÇ ‚îú‚îÄ‚îÄ index.html ‚îÇ ‚îî‚îÄ‚îÄ favicon.ico ‚îú‚îÄ‚îÄ src/ ‚îÇ ‚îú‚îÄ‚îÄ App.js # Main app component ‚îÇ ‚îú‚îÄ‚îÄ index.js # Entry point ‚îÇ ‚îú‚îÄ‚îÄ components/ ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ChatInterface.js # Chat UI component ‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ MessageList.js # Message display ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ InputBox.js # User input ‚îÇ ‚îú‚îÄ‚îÄ services/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ api.js # API calls to backend ‚îÇ ‚îú‚îÄ‚îÄ utils/ ‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ constants.js # Configuration ‚îÇ ‚îî‚îÄ‚îÄ styles/ ‚îÇ ‚îî‚îÄ‚îÄ App.css ‚îú‚îÄ‚îÄ package.json ‚îî‚îÄ‚îÄ .env Step 4: Configure Environment Variables Create .env file in the project root:\n# .env REACT_APP_API_URL=http://localhost:8000 REACT_APP_API_BASE_PATH=/api ‚ö†Ô∏è Important: In React, environment variables must start with REACT_APP_ prefix.\nStep 5: Create API Service Create src/services/api.js:\nimport axios from \u0026#39;axios\u0026#39;; const API_URL = process.env.REACT_APP_API_URL || \u0026#39;http://localhost:8000\u0026#39;; const api = axios.create({ baseURL: API_URL, headers: { \u0026#39;Content-Type\u0026#39;: \u0026#39;application/json\u0026#39;, }, }); export const sendMessage = async (sessionId, message) =\u0026gt; { try { const response = await api.post(\u0026#39;/api/chat\u0026#39;, { session_id: sessionId, message: message, }); return response.data; } catch (error) { console.error(\u0026#39;API Error:\u0026#39;, error); throw error; } }; export default api; Step 6: Create Chat Interface Component Create src/components/ChatInterface.js:\nimport React, { useState, useEffect, useRef } from \u0026#39;react\u0026#39;; import { Box, VStack, HStack, Input, Button, Text, Container, Heading, } from \u0026#39;@chakra-ui/react\u0026#39;; import ReactMarkdown from \u0026#39;react-markdown\u0026#39;; import { sendMessage } from \u0026#39;../services/api\u0026#39;; function ChatInterface() { const [messages, setMessages] = useState([]); const [input, setInput] = useState(\u0026#39;\u0026#39;); const [loading, setLoading] = useState(false); const [sessionId] = useState(() =\u0026gt; `session-${Date.now()}-${Math.random().toString(36).substr(2, 9)}` ); const messagesEndRef = useRef(null); const scrollToBottom = () =\u0026gt; { messagesEndRef.current?.scrollIntoView({ behavior: \u0026#39;smooth\u0026#39; }); }; useEffect(() =\u0026gt; { scrollToBottom(); }, [messages]); const handleSend = async () =\u0026gt; { if (!input.trim()) return; const userMessage = { role: \u0026#39;user\u0026#39;, content: input }; setMessages((prev) =\u0026gt; [...prev, userMessage]); setInput(\u0026#39;\u0026#39;); setLoading(true); try { const response = await sendMessage(sessionId, input); const assistantMessage = { role: \u0026#39;assistant\u0026#39;, content: response.response, data: response.data, }; setMessages((prev) =\u0026gt; [...prev, assistantMessage]); } catch (error) { const errorMessage = { role: \u0026#39;error\u0026#39;, content: \u0026#39;Failed to get response. Please try again.\u0026#39;, }; setMessages((prev) =\u0026gt; [...prev, errorMessage]); } finally { setLoading(false); } }; return ( \u0026lt;Container maxW=\u0026#34;container.md\u0026#34; py={8}\u0026gt; \u0026lt;VStack spacing={4} align=\u0026#34;stretch\u0026#34;\u0026gt; \u0026lt;Heading size=\u0026#34;lg\u0026#34;\u0026gt;üöó EV Rental AI Agent\u0026lt;/Heading\u0026gt; \u0026lt;Box border=\u0026#34;1px\u0026#34; borderColor=\u0026#34;gray.200\u0026#34; borderRadius=\u0026#34;lg\u0026#34; p={4} h=\u0026#34;500px\u0026#34; overflowY=\u0026#34;auto\u0026#34; bg=\u0026#34;gray.50\u0026#34; \u0026gt; \u0026lt;VStack spacing={3} align=\u0026#34;stretch\u0026#34;\u0026gt; {messages.map((msg, idx) =\u0026gt; ( \u0026lt;Box key={idx} alignSelf={msg.role === \u0026#39;user\u0026#39; ? \u0026#39;flex-end\u0026#39; : \u0026#39;flex-start\u0026#39;} maxW=\u0026#34;80%\u0026#34; bg={msg.role === \u0026#39;user\u0026#39; ? \u0026#39;blue.500\u0026#39; : \u0026#39;white\u0026#39;} color={msg.role === \u0026#39;user\u0026#39; ? \u0026#39;white\u0026#39; : \u0026#39;black\u0026#39;} p={3} borderRadius=\u0026#34;lg\u0026#34; boxShadow=\u0026#34;sm\u0026#34; \u0026gt; {msg.role === \u0026#39;assistant\u0026#39; ? ( \u0026lt;ReactMarkdown\u0026gt;{msg.content}\u0026lt;/ReactMarkdown\u0026gt; ) : ( \u0026lt;Text\u0026gt;{msg.content}\u0026lt;/Text\u0026gt; )} \u0026lt;/Box\u0026gt; ))} {loading \u0026amp;\u0026amp; ( \u0026lt;Box alignSelf=\u0026#34;flex-start\u0026#34; maxW=\u0026#34;80%\u0026#34;\u0026gt; \u0026lt;Text color=\u0026#34;gray.500\u0026#34;\u0026gt;Typing...\u0026lt;/Text\u0026gt; \u0026lt;/Box\u0026gt; )} \u0026lt;div ref={messagesEndRef} /\u0026gt; \u0026lt;/VStack\u0026gt; \u0026lt;/Box\u0026gt; \u0026lt;HStack\u0026gt; \u0026lt;Input value={input} onChange={(e) =\u0026gt; setInput(e.target.value)} onKeyPress={(e) =\u0026gt; e.key === \u0026#39;Enter\u0026#39; \u0026amp;\u0026amp; handleSend()} placeholder=\u0026#34;Ask about vehicle rentals, policies, or charging stations...\u0026#34; disabled={loading} /\u0026gt; \u0026lt;Button onClick={handleSend} colorScheme=\u0026#34;blue\u0026#34; isLoading={loading} disabled={loading} \u0026gt; Send \u0026lt;/Button\u0026gt; \u0026lt;/HStack\u0026gt; \u0026lt;/VStack\u0026gt; \u0026lt;/Container\u0026gt; ); } export default ChatInterface; Step 7: Update App.js Update src/App.js:\nimport React from \u0026#39;react\u0026#39;; import { ChakraProvider } from \u0026#39;@chakra-ui/react\u0026#39;; import ChatInterface from \u0026#39;./components/ChatInterface\u0026#39;; function App() { return ( \u0026lt;ChakraProvider\u0026gt; \u0026lt;ChatInterface /\u0026gt; \u0026lt;/ChakraProvider\u0026gt; ); } export default App; Step 8: Run the Frontend Start the React development server:\nnpm start Expected output:\nCompiled successfully! You can now view ev-rental-frontend in the browser. Local: http://localhost:3000 On Your Network: http://192.168.1.10:3000 The application will automatically open in your browser at http://localhost:3000.\nStep 9: Test the Chat Interface Try these sample queries:\nKnowledge Base Query:\n\u0026ldquo;Ch√≠nh s√°ch thu√™ xe l√† g√¨?\u0026rdquo; \u0026ldquo;T√¥i c·∫ßn gi·∫•y t·ªù g√¨ ƒë·ªÉ thu√™ xe?\u0026rdquo; Vehicle Search:\n\u0026ldquo;T√¨m xe VinFast VF8 ·ªü H√† N·ªôi t·ª´ ng√†y 20/12\u0026rdquo; \u0026ldquo;C√≥ xe n√†o available?\u0026rdquo; Charging Station:\n\u0026ldquo;Tr·∫°m s·∫°c g·∫ßn Ho√†n Ki·∫øm\u0026rdquo; \u0026ldquo;T√¨m tr·∫°m s·∫°c ·ªü Qu·∫≠n 1\u0026rdquo; Verification Checklist Before proceeding, ensure:\n‚úÖ Node.js and npm installed ‚úÖ All dependencies installed successfully ‚úÖ .env file configured with backend URL ‚úÖ Backend server running on port 8000 ‚úÖ Frontend running on port 3000 ‚úÖ Chat interface loads without errors ‚úÖ Can send messages and receive responses ‚úÖ Markdown formatting displays correctly Troubleshooting Issue: \u0026ldquo;Module not found\u0026rdquo;\nSolution: Delete node_modules and run npm install again Check package.json for correct versions Issue: \u0026ldquo;Network Error\u0026rdquo; when sending messages\nCheck backend is running: curl http://localhost:8000/health Verify REACT_APP_API_URL in .env Check browser console for CORS errors Issue: \u0026ldquo;CORS policy error\u0026rdquo;\nEnsure backend has CORS middleware configured Check allow_origins includes http://localhost:3000 Issue: Port 3000 already in use\nChange port: PORT=3001 npm start Or kill existing process Issue: Markdown not rendering\nVerify react-markdown is installed Check import statement in ChatInterface.js Next: Proceed to Testing to verify all features work correctly.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.5-policy/","title":"VPC Endpoint Policies","tags":[],"description":"","content":"When you create an interface or gateway endpoint, you can attach an endpoint policy to it that controls access to the service to which you are connecting. A VPC endpoint policy is an IAM resource policy that you attach to an endpoint. If you do not attach a policy when you create an endpoint, AWS attaches a default policy for you that allows full access to the service through the endpoint.\nYou can create a policy that restricts access to specific S3 buckets only. This is useful if you only want certain S3 Buckets to be accessible through the endpoint.\nIn this section you will create a VPC endpoint policy that restricts access to the S3 bucket specified in the VPC endpoint policy.\nConnect to an EC2 instance and verify connectivity to S3 Start a new AWS Session Manager session on the instance named Test-Gateway-Endpoint. From the session, verify that you can list the contents of the bucket you created in Part 1: Access S3 from VPC: aws s3 ls s3://\\\u0026lt;your-bucket-name\\\u0026gt; The bucket contents include the two 1 GB files uploaded in earlier.\nCreate a new S3 bucket; follow the naming pattern you used in Part 1, but add a \u0026lsquo;-2\u0026rsquo; to the name. Leave other fields as default and click create Successfully create bucket\nNavigate to: Services \u0026gt; VPC \u0026gt; Endpoints, then select the Gateway VPC endpoint you created earlier. Click the Policy tab. Click Edit policy. The default policy allows access to all S3 Buckets through the VPC endpoint.\nIn Edit Policy console, copy \u0026amp; Paste the following policy, then replace yourbucketname-2 with your 2nd bucket name. This policy will allow access through the VPC endpoint to your new bucket, but not any other bucket in Amazon S3. Click Save to apply the policy. { \u0026#34;Id\u0026#34;: \u0026#34;Policy1631305502445\u0026#34;, \u0026#34;Version\u0026#34;: \u0026#34;2012-10-17\u0026#34;, \u0026#34;Statement\u0026#34;: [ { \u0026#34;Sid\u0026#34;: \u0026#34;Stmt1631305501021\u0026#34;, \u0026#34;Action\u0026#34;: \u0026#34;s3:*\u0026#34;, \u0026#34;Effect\u0026#34;: \u0026#34;Allow\u0026#34;, \u0026#34;Resource\u0026#34;: [ \u0026#34;arn:aws:s3:::yourbucketname-2\u0026#34;, \u0026#34;arn:aws:s3:::yourbucketname-2/*\u0026#34; ], \u0026#34;Principal\u0026#34;: \u0026#34;*\u0026#34; } ] } Successfully customize policy\nFrom your session on the Test-Gateway-Endpoint instance, test access to the S3 bucket you created in Part 1: Access S3 from VPC aws s3 ls s3://\u0026lt;yourbucketname\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy:\nReturn to your home directory on your EC2 instance cd~ Create a file fallocate -l 1G test-bucket2.xyz Copy file to 2nd bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-2nd-bucket-name\u0026gt; This operation succeeds because it is permitted by the VPC endpoint policy.\nThen we test access to the first bucket by copy the file to 1st bucket aws s3 cp test-bucket2.xyz s3://\u0026lt;your-1st-bucket-name\u0026gt; This command will return an error because access to this bucket is not permitted by your new VPC endpoint policy.\nPart 3 Summary: In this section, you created a VPC endpoint policy for Amazon S3, and used the AWS CLI to test the policy. AWS CLI actions targeted to your original S3 bucket failed because you applied a policy that only allowed access to the second bucket you created. AWS CLI actions targeted for your second bucket succeeded because the policy allowed them. These policies can be useful in situations where you need to control access to resources through VPC endpoints.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.5-week5/","title":"Week 5 Worklog","tags":[],"description":"","content":"Week 5 Objectives: Fast VPS: Deploy a complete WordPress application in 5 minutes using Amazon Lightsail. Containerization: Package a small application into a Docker Image and deploy on Lightsail Container Service. Elasticity: Set up self-healing and auto-scaling architecture with EC2 Auto Scaling Group. Load Balancing: Distribute user traffic through Application Load Balancer (ALB). Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T5.1 2 Lightsail - WordPress Deploy: - Initialize Lightsail Instance with \u0026ldquo;WordPress\u0026rdquo; Blueprint - Assign Static IP 10/06/2025 10/06/2025 Completed https://cloudjourney.awsstudygroup.com/ T5.2 2 Container - Docker Intro: - Write simple Dockerfile for Hello World webpage - Build and test on Cloud9 10/06/2025 10/07/2025 Completed https://cloudjourney.awsstudygroup.com/ T5.3 3 Lightsail - Container Deploy: - Push Docker Image to Lightsail Container Service - Configure Public Endpoint 10/07/2025 10/08/2025 Completed https://cloudjourney.awsstudygroup.com/ T5.4 4 EC2 - Launch Template: - Create Launch Template: Amazon Linux 2023, t3.micro - User Data script to auto-install Apache Web Server 10/08/2025 10/09/2025 Completed https://cloudjourney.awsstudygroup.com/ T5.5 4 Networking - Target Group: - Create empty Target Group - Ready for ASG to register instances 10/09/2025 10/10/2025 Completed https://cloudjourney.awsstudygroup.com/ T5.6 5 EC2 - Create ALB: - Initialize internet-facing Application Load Balancer - Listen on port 80, route traffic to Target Group 10/09/2025 10/10/2025 Completed https://cloudjourney.awsstudygroup.com/ T5.7 5 EC2 - Auto Scaling Group: - Create ASG: Min=1, Desired=2, Max=4 - Use Launch Template - Integrate with ALB Target Group 10/09/2025 10/10/2025 Completed https://cloudjourney.awsstudygroup.com/ T5.8 6 Testing - Stress Test: - SSH into instance, install stress tool - Push CPU to 100% and observe ASG Scale Out 10/10/2025 10/12/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 5 Achievements: Comparison:\nLightsail extremely fast to deploy but lacks deep network customization Lightsail VPC Peering has limitations Suitable for small projects, blogs, prototypes Elastic System:\nBuilt a Fault Tolerant Web system When manually terminating 1 instance, ASG immediately creates a new replacement instance System automatically scales up/down based on demand Load Balancing:\nALB distributes requests evenly between instances Ensures users don\u0026rsquo;t experience service interruption when a server fails Understanding of Health Checks and Drain Connection Architecture:\nSolid grasp of Stateless Architecture concept Cannot store uploaded files or sessions on EC2 hard drive Combine S3 (files) and RDS (data) for Cloud-Native system User Data for Bootstrapping is a key automation technique "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/","title":"Workshop","tags":[],"description":"","content":"Building an EV Rental AI Agent with AWS Bedrock Overview EV Rental AI Agent is an intelligent chatbot built to assist customers in the VinFast electric vehicle rental system. This workshop demonstrates how to leverage AWS Bedrock, Claude 3.5 Sonnet, and Knowledge Bases to create a conversational AI that can:\nAnswer natural language questions in Vietnamese Automatically search information from multiple sources Display data as interactive cards in the chat interface Retrieve available vehicles and charging stations Access rental policies and FAQs from a knowledge base In this workshop, you will learn how to:\nSetup AWS Bedrock - Enable AI models and create a Knowledge Base for document retrieval Deploy Backend API - Build a FastAPI server with Strands Agent SDK for intelligent tool selection Deploy Frontend - Create a React chat interface with Chakra UI components Test the System - Interact with the AI agent and verify all functionalities Content Workshop Overview Prerequisites Setup AWS Bedrock Deploy Backend API Deploy Frontend Testing the AI Agent Clean Up Resources "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.6-cleanup/","title":"Clean up","tags":[],"description":"","content":"Congratulations on completing this workshop! In this workshop, you learned architecture patterns for accessing Amazon S3 without using the Public Internet.\nBy creating a gateway endpoint, you enabled direct communication between EC2 resources and Amazon S3, without traversing an Internet Gateway. By creating an interface endpoint you extended S3 connectivity to resources running in your on-premises data center via AWS Site-to-Site VPN or Direct Connect. clean up Navigate to Hosted Zones on the left side of Route 53 console. Click the name of s3.us-east-1.amazonaws.com zone. Click Delete and confirm deletion by typing delete. Disassociate the Route 53 Resolver Rule - myS3Rule from \u0026ldquo;VPC Onprem\u0026rdquo; and Delete it. Open the CloudFormation console and delete the two CloudFormation Stacks that you created for this lab: PLOnpremSetup PLCloudSetup Delete S3 buckets Open S3 console Choose the bucket we created for the lab, click and confirm empty. Click delete and confirm delete. "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/6-self-evaluation/","title":"Self-Assessment","tags":[],"description":"","content":"During my internship at AMAZON WEB SERVICES VIETNAM COMPANY LIMITED from 09/08 to [end date], I had the opportunity to learn, practice, and apply the knowledge acquired in school to a real-world working environment.\nI participated in the VoltGo - EV Station-based Rental System project as a Frontend (FE) developer. This is a green mobility solution for smart cities, aiming to provide an \u0026ldquo;all-in-one\u0026rdquo; cloud-based platform that unifies searching, real-time booking, and payment, addressing the current service fragmentation.\nThrough my participation in project development, I have accumulated and significantly expanded my professional skill set, including:\nFrontend and Application Development: I have become proficient in building cross-platform mobile applications using React Native and developing Web Dashboard management interfaces with Next.js. Additionally, I have practical experience in deploying and hosting web interfaces through AWS Amplify.\nUnderstanding of Cloud Architecture \u0026amp; Backend: Although focused on Frontend, I have grasped the operational workflow of the Spring Boot Backend system and its integration with the comprehensive AWS ecosystem.\nI have approached and clearly understood the role of core services in the project, including:\nCompute \u0026amp; Containers: Amazon ECS Fargate operates the backend and AWS Lambda for serverless processing.\nStorage \u0026amp; Databases: Amazon S3 (Data Lake), Amazon RDS PostgreSQL (relational storage), and Amazon ElastiCache for Redis (caching).\nConnectivity \u0026amp; IoT: AWS IoT Core (receiving sensor data), Amazon API Gateway (API management), and Amazon Cognito (secure user authentication).\nData Processing \u0026amp; Distribution: AWS Glue (ETL and Crawlers) and Amazon CloudFront (global content delivery network).\nSupporting Tools: Using AWS CDK/SDK during infrastructure development and deployment.\nIn my work, I always uphold a sense of responsibility, strive to complete all assigned targets, and comply with organizational regulations. Additionally, I proactively collaborate closely with colleagues to ensure smooth workflow and optimal efficiency.\nTo objectively reflect on my internship process, I would like to evaluate myself based on the following criteria:\nNo. Criteria Description Good Fair Average 1 Professional knowledge \u0026amp; skills Understanding of the field, applying knowledge in practice, proficiency with tools, work quality ‚òê ‚úÖ ‚òê 2 Ability to learn Ability to absorb new knowledge and learn quickly ‚òê ‚úÖ ‚òê 3 Proactiveness Taking initiative, seeking out tasks without waiting for instructions ‚úÖ ‚òê ‚òê 4 Sense of responsibility Completing tasks on time and ensuring quality ‚úÖ ‚òê ‚òê 5 Discipline Adhering to schedules, rules, and work processes ‚òê ‚úÖ ‚òê 6 Progressive mindset Willingness to receive feedback and improve oneself ‚úÖ ‚òê ‚òê 7 Communication Presenting ideas and reporting work clearly ‚úÖ ‚òê ‚òê 8 Teamwork Working effectively with colleagues and participating in teams ‚úÖ ‚òê ‚òê 9 Professional conduct Respecting colleagues, partners, and the work environment ‚úÖ ‚òê ‚òê 10 Problem-solving skills Identifying problems, proposing solutions, and showing creativity ‚òê ‚úÖ ‚òê 11 Contribution to project/team Work effectiveness, innovative ideas, recognition from the team ‚úÖ ‚òê ‚òê 12 Overall General evaluation of the entire internship period ‚úÖ ‚òê ‚òê Needs Improvement Through my work experience at Amazon Web Services Vietnam, I recognize that I still have some limitations that need to be addressed:\nRegarding discipline:\nCurrent situation: Sometimes I am not strict enough with myself in absolutely adhering to administrative procedures or schedules.\nPlan: I will cultivate stricter time management habits and commit to strictly complying with company regulations as well as any organization I join in the future.\nRegarding problem-solving thinking:\nCurrent situation: When encountering complex technical issues in the project (e.g., real-time data processing or AWS integration), I sometimes feel confused and don\u0026rsquo;t have an optimal solution immediately.\nPlan: I will develop the habit of analyzing root causes and thoroughly studying technical documentation (AWS Well-Architected Framework) to propose more systematic solutions.\nRegarding communication skills:\nCurrent situation: Expressing technical ideas or daily progress reports is sometimes not concise enough.\nPlan: I will learn to present issues more briefly and succinctly, and actively participate in group discussions to practice handling work situations.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.6-testing/","title":"Testing the System","tags":[],"description":"","content":"Testing the EV Rental AI Agent In this section, you will test all three core features of the AI Agent to ensure everything works correctly.\nPrerequisites for Testing Before testing, ensure:\n‚úÖ Backend server running on http://localhost:8000 ‚úÖ Frontend application running on http://localhost:3000 ‚úÖ PostgreSQL database is running and populated with test data ‚úÖ AWS Bedrock Knowledge Base is synced and ready Test Scenario 1: Knowledge Base Search The AI Agent should be able to answer questions about rental policies, pricing, and FAQs using the Knowledge Base.\nTest Queries:\nRental Policy:\nUser: \u0026#34;Ch√≠nh s√°ch thu√™ xe l√† g√¨?\u0026#34; Expected: Agent returns rental policy details from Knowledge Base Required Documents:\nUser: \u0026#34;T√¥i c·∫ßn gi·∫•y t·ªù g√¨ ƒë·ªÉ thu√™ xe?\u0026#34; Expected: Agent lists required documents (ID, license, deposit info) Pricing Information:\nUser: \u0026#34;Gi√° thu√™ xe VinFast VF8 l√† bao nhi√™u?\u0026#34; Expected: Agent provides pricing details from Knowledge Base Booking Process:\nUser: \u0026#34;L√†m th·∫ø n√†o ƒë·ªÉ ƒë·∫∑t xe?\u0026#34; Expected: Agent explains step-by-step booking process Verification:\n‚úÖ Response includes citation from Knowledge Base ‚úÖ Answer is relevant and accurate ‚úÖ Markdown formatting displays correctly ‚úÖ Response time is under 5 seconds Test Scenario 2: Vehicle Search The AI Agent should search the PostgreSQL database for available vehicles based on location and date.\nTest Queries:\nSearch by Location:\nUser: \u0026#34;T√¨m xe ·ªü H√† N·ªôi\u0026#34; Expected: Agent lists available vehicles in Hanoi Search by Model:\nUser: \u0026#34;C√≥ xe VinFast VF8 n√†o available kh√¥ng?\u0026#34; Expected: Agent shows VF8 vehicles with availability status Search with Date Range:\nUser: \u0026#34;T√¨m xe VF9 ·ªü H·ªì Ch√≠ Minh t·ª´ ng√†y 20/12 ƒë·∫øn 25/12\u0026#34; Expected: Agent searches vehicles available in that date range Search with Price Range:\nUser: \u0026#34;Xe n√†o d∆∞·ªõi 1 tri·ªáu ƒë·ªìng/ng√†y?\u0026#34; Expected: Agent filters vehicles by price Verification:\n‚úÖ Agent correctly extracts search parameters (location, model, dates) ‚úÖ Results include vehicle details (model, price, location, availability) ‚úÖ Data is fetched from PostgreSQL database ‚úÖ Results are formatted in a readable table or list Expected Response Format:\n## üöó Available Vehicles | Model | Location | Price/Day | Status | |-------|----------|-----------|--------| | VinFast VF8 | H√† N·ªôi | 800,000ƒë | Available | | VinFast VF9 | H√† N·ªôi | 1,200,000ƒë | Available | Test Scenario 3: Charging Station Finder The AI Agent should find nearby charging stations with real-time availability.\nTest Queries:\nSearch by District:\nUser: \u0026#34;Tr·∫°m s·∫°c g·∫ßn Qu·∫≠n Ho√†n Ki·∫øm\u0026#34; Expected: Agent lists charging stations in Hoan Kiem district Search by Address:\nUser: \u0026#34;T√¨m tr·∫°m s·∫°c ·ªü Qu·∫≠n 1, TP.HCM\u0026#34; Expected: Agent finds stations in District 1, HCMC Check Station Availability:\nUser: \u0026#34;Tr·∫°m s·∫°c n√†o c√≤n tr·ªëng?\u0026#34; Expected: Agent shows stations with available charging ports Filter by Connector Type:\nUser: \u0026#34;Tr·∫°m s·∫°c c√≥ CCS2 connector\u0026#34; Expected: Agent filters stations with CCS2 connectors Verification:\n‚úÖ Agent correctly identifies location from query ‚úÖ Results include station name, address, and availability ‚úÖ Connector types are listed ‚úÖ Real-time availability status is shown Expected Response Format:\n## ‚ö° Charging Stations Near You ### VinFast Station - Ho√†n Ki·∫øm üìç Address: 123 Tr·∫ßn H∆∞ng ƒê·∫°o, Ho√†n Ki·∫øm, H√† N·ªôi üîå Connectors: CCS2 (2 available), CHAdeMO (1 available) ‚è∞ Hours: 24/7 ‚úÖ Status: Available Test Scenario 4: Multi-Turn Conversations Test the agent\u0026rsquo;s ability to maintain context across multiple turns.\nTest Conversation:\nUser: \u0026#34;T√¥i mu·ªën thu√™ xe VF8\u0026#34; Agent: [Provides VF8 information] User: \u0026#34;Gi√° bao nhi√™u?\u0026#34; Agent: [Should understand context refers to VF8 pricing] User: \u0026#34;Tr·∫°m s·∫°c g·∫ßn ƒë√≥ ·ªü ƒë√¢u?\u0026#34; Agent: [Should find charging stations near VF8 location] Verification:\n‚úÖ Agent maintains conversation context ‚úÖ Pronouns and references are resolved correctly ‚úÖ Session ID persists across messages Test Scenario 5: Error Handling Test how the agent handles invalid or unclear queries.\nTest Cases:\nAmbiguous Query:\nUser: \u0026#34;Xe\u0026#34; Expected: Agent asks for clarification Unavailable Vehicle:\nUser: \u0026#34;T√¨m xe Tesla\u0026#34; Expected: Agent explains Tesla is not available, suggests alternatives Invalid Date:\nUser: \u0026#34;Thu√™ xe t·ª´ ng√†y 32/13\u0026#34; Expected: Agent detects invalid date and asks for correction Out of Scope:\nUser: \u0026#34;What\u0026#39;s the weather today?\u0026#34; Expected: Agent politely explains it can only help with EV rentals Verification:\n‚úÖ Agent handles errors gracefully ‚úÖ Provides helpful error messages ‚úÖ Suggests alternatives when possible Performance Testing Check system performance under normal usage:\nMetrics to Monitor:\nResponse Time:\nKnowledge Base queries: \u0026lt; 3 seconds Vehicle search: \u0026lt; 2 seconds Charging station search: \u0026lt; 2 seconds API Health:\ncurl http://localhost:8000/health Expected: 200 OK with health status\nBackend Logs: Check for errors in FastAPI console output\nFrontend Console: Open browser DevTools ‚Üí Console\nNo JavaScript errors API calls succeed (Network tab) Integration Testing Checklist Run through this comprehensive checklist:\n‚úÖ Knowledge Base Integration:\nAgent can retrieve policy information Citations are included in responses Bedrock API calls succeed ‚úÖ Database Integration:\nVehicle search queries PostgreSQL Results are accurate and up-to-date Database connection is stable ‚úÖ Backend API:\n/api/chat endpoint works /health endpoint responds Session management functions correctly ‚úÖ Frontend UI:\nMessages display correctly User input is captured Loading states work Markdown renders properly Auto-scroll functions ‚úÖ Error Handling:\nNetwork errors are caught Invalid inputs handled gracefully User receives helpful feedback Testing with Postman (Optional) Test backend API directly:\n1. Health Check:\nGET http://localhost:8000/health 2. Chat Request:\nPOST http://localhost:8000/api/chat Content-Type: application/json { \u0026#34;session_id\u0026#34;: \u0026#34;test-session-123\u0026#34;, \u0026#34;message\u0026#34;: \u0026#34;Ch√≠nh s√°ch thu√™ xe l√† g√¨?\u0026#34; } Expected Response:\n{ \u0026#34;response\u0026#34;: \u0026#34;## üìã Ch√≠nh s√°ch thu√™ xe VinFast\\n\\n...\u0026#34;, \u0026#34;data\u0026#34;: null, \u0026#34;session_id\u0026#34;: \u0026#34;test-session-123\u0026#34; } Troubleshooting Test Failures Issue: Knowledge Base returns empty results\nCheck Knowledge Base is synced in AWS Console Verify KNOWLEDGE_BASE_ID in .env Test KB directly in Bedrock console Issue: Vehicle search returns no results\nCheck PostgreSQL database has test data Verify DATABASE_URL connection string Run SQL query directly: SELECT * FROM vehicles; Issue: Charging stations not found\nVerify backend API /stations endpoint works Check station data in database Test API call: curl http://localhost:8080/stations Issue: Frontend not connecting to backend\nCheck REACT_APP_API_URL in frontend .env Verify backend CORS allows http://localhost:3000 Check browser console for network errors Test Report Template Document your test results:\n## Test Report - EV Rental AI Agent **Date:** 2024-12-20 **Tester:** Your Name ### Test Results Summary - Total Tests: 15 - Passed: 14 - Failed: 1 - Success Rate: 93% ### Detailed Results #### Knowledge Base Search - [x] Rental policy query - PASS - [x] Required documents - PASS - [x] Pricing information - PASS - [ ] Booking process - FAIL (slow response) #### Vehicle Search - [x] Search by location - PASS - [x] Search by model - PASS - [x] Date range search - PASS #### Charging Station Finder - [x] District search - PASS - [x] Availability check - PASS ### Issues Found 1. Booking process query takes 7 seconds (\u0026gt; 5s threshold) - Root cause: Knowledge Base sync incomplete - Fix: Re-sync data source ### Recommendations - Monitor response times during peak usage - Add caching for frequently asked questions - Implement rate limiting Success! üéâ Your EV Rental AI Agent is now fully tested and operational.\nNext: Proceed to Cleanup to remove resources and avoid charges.\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.6-week6/","title":"Week 6 Worklog","tags":[],"description":"","content":"Week 6 Objectives: Observability: Build a centralized Dashboard to monitor system health. Proactive Alerts: Set up alerting system via Email/SMS when resources encounter issues. Automation: Write Lambda function (Python/Node.js) to interact with AWS resources. Scheduling: Use Amazon EventBridge to trigger Lambda on schedule (Cron job). Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T6.1 2 CloudWatch - Dashboard: - Create Dashboard displaying CPU Utilization of ASG - Display Freeable Memory of RDS 10/13/2025 10/13/2025 Completed https://cloudjourney.awsstudygroup.com/ T6.2 2 SNS - Setup Topic: - Create SNS Topic DevOps-Alerts - Subscribe personal email and confirm 10/13/2025 10/14/2025 Completed https://cloudjourney.awsstudygroup.com/ T6.3 3 CloudWatch - Create Alarm: - Create Alarm: CPU \u0026gt; 70% for 2 consecutive periods - Trigger SNS Topic to send email 10/14/2025 10/14/2025 Completed https://cloudjourney.awsstudygroup.com/ T6.4 4 Lambda - IAM Role: - Create IAM Role for Lambda - Permission: AmazonEC2FullAccess (note: should limit to Start/Stop) 10/15/2025 10/15/2025 Completed https://cloudjourney.awsstudygroup.com/ T6.5 4 Lambda - Coding: - Write Python function (boto3) - List running instances and execute stop_instances 10/15/2025 10/16/2025 Completed https://cloudjourney.awsstudygroup.com/ T6.6 5 EventBridge - Scheduler: - Create Rule \u0026ldquo;Cost-Saver\u0026rdquo; - Cron: 0 18 * * ? * (6:00 PM daily) - Trigger Lambda 10/16/2025 10/16/2025 Completed https://cloudjourney.awsstudygroup.com/ T6.7 6 Testing \u0026amp; Review: - Verify Lambda function operates correctly on schedule - Review CloudWatch Logs of Lambda - Compile monitoring report for this week 10/17/2025 10/17/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 6 Achievements: Vision:\nReal-time view of system performance No longer need to SSH into each machine to run top command Centralized dashboard for all resources Response:\nReceive immediate email alert when CPU is high Re-tested with Stress Test from Week 5 Alert system works well FinOps:\nAutomatically shut down Development environment at end of day Save approximately 65% EC2 cost (8h instead of 24h) Lambda runs at nearly $0 cost Skills:\nWrite Lambda function with Python and boto3 Understanding of Event-Driven Architecture Configure EventBridge (CloudWatch Events) Integrate SNS for notification system "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/5-workshop/5.7-cleanup/","title":"Cleanup Resources","tags":[],"description":"","content":"Cleaning Up Resources After completing the workshop, follow these steps to clean up all resources and avoid unnecessary AWS charges.\nWhy Cleanup is Important Cost Savings: AWS charges for active resources like Bedrock Knowledge Bases, S3 storage, and running services Security: Remove unused IAM credentials to maintain security best practices Organization: Keep your AWS account clean and organized Step 1: Delete AWS Bedrock Knowledge Base 1.1 Delete Knowledge Base Open the AWS Bedrock Console Navigate to Knowledge Bases in the left sidebar Select your Knowledge Base: ev-rental-knowledge-base Click Delete Confirm deletion by typing the Knowledge Base name Click Delete to confirm ‚ö†Ô∏è Note: This will also delete the associated data source connections.\n1.2 Delete S3 Bucket and Documents Open the S3 Console Find your bucket: ev-rental-knowledge-docs Select the bucket Click Empty to delete all objects Confirm by typing \u0026ldquo;permanently delete\u0026rdquo; After emptying, click Delete on the bucket Confirm by typing the bucket name Or use AWS CLI:\n# Delete all objects in bucket aws s3 rm s3://ev-rental-knowledge-docs --recursive # Delete the bucket aws s3 rb s3://ev-rental-knowledge-docs Step 2: Delete IAM User and Access Keys 2.1 Delete Access Keys Open the IAM Console Navigate to Users Select your user (e.g., bedrock-agent-user) Click on the Security credentials tab Under Access keys, find your access key Click Delete next to the access key Confirm deletion 2.2 Delete IAM User (Optional) If you created a dedicated IAM user for this workshop:\nIn the IAM Console, select the user Click Delete user Confirm by checking the box Click Delete Or use AWS CLI:\n# List access keys aws iam list-access-keys --user-name bedrock-agent-user # Delete access key (replace with your key ID) aws iam delete-access-key --user-name bedrock-agent-user --access-key-id AKIA5GPEMGJZK6E7PMEB # Delete user aws iam delete-user --user-name bedrock-agent-user Step 3: Stop Local Services 3.1 Stop FastAPI Backend In the terminal where FastAPI is running:\nPress Ctrl + C to stop the server\nDeactivate virtual environment:\ndeactivate Optionally delete the project folder:\n# On macOS/Linux rm -rf ev-rental-backend # On Windows rmdir /s ev-rental-backend 3.2 Stop React Frontend In the terminal where React is running:\nPress Ctrl + C to stop the development server\nOptionally delete the project folder:\n# On macOS/Linux rm -rf ev-rental-frontend # On Windows rmdir /s ev-rental-frontend 3.3 Stop PostgreSQL Database If you installed PostgreSQL locally for this workshop:\nOn macOS:\n# Stop PostgreSQL service brew services stop postgresql@14 On Linux:\nsudo systemctl stop postgresql On Windows:\n# Open Services (services.msc) # Find \u0026#34;PostgreSQL\u0026#34; service # Right-click ‚Üí Stop 3.4 Delete Database (Optional) If you want to completely remove the database:\n# Connect to PostgreSQL psql -U postgres # Drop database DROP DATABASE ev_rental_db; # Exit \\q Step 4: Remove Environment Files Delete sensitive .env files that contain credentials:\nBackend:\ncd ev-rental-backend rm .env Frontend:\ncd ev-rental-frontend rm .env ‚ö†Ô∏è Security Note: Never commit .env files to Git. Always add them to .gitignore.\nStep 5: Verify Cleanup 5.1 Check AWS Resources Verify all resources are deleted:\nBedrock Console:\nNo Knowledge Bases listed No model invocations active S3 Console:\nBucket ev-rental-knowledge-docs is deleted IAM Console:\nAccess keys are deleted IAM user removed (if you chose to delete it) 5.2 Check AWS Costs Open the AWS Billing Console Check Bills for the current month Verify charges: Bedrock charges should stop after Knowledge Base deletion S3 storage charges should stop after bucket deletion No ongoing compute charges Or use AWS CLI:\naws ce get-cost-and-usage \\ --time-period Start=2024-12-01,End=2024-12-31 \\ --granularity MONTHLY \\ --metrics UnblendedCost \\ --group-by Type=SERVICE Cost Breakdown Here\u0026rsquo;s what you may have been charged during the workshop:\nService Estimated Cost Notes AWS Bedrock - Claude 3.5 Sonnet ~$0.50 - $2.00 Depends on number of queries AWS Bedrock - Knowledge Base ~$0.10 - $0.50 Vector storage and retrieval S3 Storage ~$0.02 Minimal for small documents Data Transfer ~$0.05 Usually within free tier Total ~$0.67 - $2.57 Approximate for workshop ‚ö†Ô∏è Note: Most costs come from Bedrock API calls. The longer you test, the higher the cost.\nCleanup Checklist Before you finish, verify all items are complete:\nAWS Resources ‚úÖ Bedrock Knowledge Base deleted ‚úÖ S3 bucket emptied and deleted ‚úÖ IAM Access Keys deleted ‚úÖ IAM User deleted (optional) Local Resources ‚úÖ FastAPI backend stopped ‚úÖ React frontend stopped ‚úÖ PostgreSQL database stopped ‚úÖ PostgreSQL database dropped (optional) Sensitive Files ‚úÖ Backend .env file deleted ‚úÖ Frontend .env file deleted ‚úÖ No AWS credentials in project files Verification ‚úÖ AWS Console shows no active resources ‚úÖ Billing dashboard shows stopped charges ‚úÖ Local services not running Troubleshooting Cleanup Issues Issue: Cannot delete S3 bucket - \u0026ldquo;Bucket not empty\u0026rdquo;\nSolution: Empty all objects first using the S3 Console or CLI Command: aws s3 rm s3://bucket-name --recursive Issue: Cannot delete Knowledge Base - \u0026ldquo;In use\u0026rdquo;\nSolution: Wait a few minutes for any pending operations to complete Check if any API calls are still referencing it Issue: IAM User deletion fails - \u0026ldquo;User has attached policies\u0026rdquo;\nSolution: Detach all policies first Go to IAM ‚Üí Users ‚Üí Select user ‚Üí Permissions ‚Üí Detach policies Issue: PostgreSQL won\u0026rsquo;t stop\nSolution: Force kill the process On macOS/Linux: sudo killall postgres On Windows: Use Task Manager to end PostgreSQL processes Optional: Keep Learning If you want to continue experimenting:\nKeep These Resources: ‚úÖ IAM User (with minimal permissions) ‚úÖ Bedrock model access (no charge when not in use) What You Can Do Next: Add more documents to Knowledge Base Implement additional agent tools Deploy to AWS Lambda for serverless operation Add authentication and user management Integrate with real vehicle booking systems Conclusion üéâ Congratulations! You have successfully:\n‚úÖ Built an AI Agent using AWS Bedrock and Claude 3.5 Sonnet ‚úÖ Integrated Knowledge Bases for intelligent document retrieval ‚úÖ Created a FastAPI backend with Strands Agent SDK ‚úÖ Developed a React frontend for user interaction ‚úÖ Tested all features end-to-end ‚úÖ Cleaned up resources to avoid charges Key Takeaways: AI Agents can autonomously select tools and make decisions AWS Bedrock simplifies access to foundation models like Claude Knowledge Bases enable semantic search over documents Strands SDK provides a framework for building agent workflows FastAPI + React create a modern full-stack AI application Next Steps: Explore other Bedrock models (Llama 3, Mistral, etc.) Learn about RAG (Retrieval Augmented Generation) Build more complex agent workflows Deploy to production using AWS services "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/7-feedback/","title":"Sharing and Feedback","tags":[],"description":"","content":" Here, you can freely share your personal opinions about your experience participating in the First Cloud Journey program, helping the FCJ team improve any shortcomings based on the following aspects:\nOverall Evaluation 1. Working Environment\nThe environment at FCJ is very professional, dynamic, and technology-focused. What impressed me most is the \u0026ldquo;Open communication\u0026rdquo; culture ‚Äì where all opinions are heard. The workspace stimulates creativity, especially with cloud infrastructure and resources always available for interns to experience. However, sometimes high project pressure creates a somewhat tense atmosphere. I think having additional short break-time activities would help everyone recharge better.\n2. Support from Mentor / Team Admin\nThe mentor provides very detailed guidance, explains clearly when I don\u0026rsquo;t understand, and always encourages me to ask questions. The admin team supports procedures, documentation, and creates favorable conditions for me to work smoothly. I highly appreciate that the mentor allows me to try and solve problems myself instead of just giving answers.\n3. Relevance of Work to Academic Major\nThe program helped me expand my comfort zone, from only knowing how to write frontend code to understanding how the frontend interacts with backend systems and cloud infrastructure. This is valuable supplementary knowledge that schools rarely have the opportunity to delve into deeply.\n4. Learning \u0026amp; Skill Development Opportunities\nBesides technical skills (React Native, AWS Services), I learned a lot about Agile/Scrum software development processes and professional source code management. Especially through code reviews and progress meetings, I realized the importance of communication and presentation skills ‚Äì areas I am striving to improve daily.\n5. Company Culture \u0026amp; Team Spirit\nThe company culture is very positive: everyone respects each other, works seriously but remains cheerful. The spirit of \u0026ldquo;Ownership\u0026rdquo; is highly valued. All members support each other wholeheartedly. Even when I made mistakes due to not following procedures, instead of criticism, everyone sat down together to help me find solutions. This made me feel accepted and motivated to change positively.\n6. Internship Policies / Benefits\nThe rights regarding internship certification support are very clear. The biggest advantage is being provided an AWS account to practice freely without worrying about costs ‚Äì an extremely practical benefit for technical people. Additionally, being able to participate in internal training sessions is a major plus.\nAdditional Questions What did you find most satisfying during your internship?\nIt was the opportunity to \u0026ldquo;do real work\u0026rdquo;. I was directly involved in building a complex product and saw my code running in reality on devices, not just limited to simulated exercises. What do you think the company should improve for future interns?\nI think the company should organize more in-depth soft skills workshops from the beginning of the term, especially on Time Management and Work Discipline. For new students like me, transitioning from an academic environment to a professional work environment sometimes causes \u0026ldquo;culture shock\u0026rdquo; regarding schedules and processes. If recommending to a friend, would you suggest they intern here? Why or why not?\nDefinitely yes. This is an ideal environment to \u0026ldquo;push\u0026rdquo; yourself to mature. You will not only learn cutting-edge technology (Cloud, IoT) but also cultivate the serious work attitude of a true software engineer. Suggestions \u0026amp; Expectations Do you have any suggestions to improve the internship experience?\nI suggest the program could establish \u0026ldquo;Check-point\u0026rdquo; milestones for discipline and attitude every 2 weeks (instead of only evaluating technical skills). Early feedback on work style will help interns adjust their behavior in time, avoiding procedural errors at the end of the term. Would you like to continue this program in the future?\nI very much hope to have the opportunity to continue working with FCJ or AWS in higher positions (such as Fresher or Junior) after overcoming my current weaknesses. Any other comments (free sharing):\nThank you FCJ for giving me a memorable journey. Although I still have many shortcomings to improve, the patience of my Mentor and Team has been the greatest motivation for me to persevere to the end. "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.7-week7/","title":"Week 7 Worklog","tags":[],"description":"","content":"Week 7 Objectives: Access Security: Eliminate the need for SSH Keys and Port 22 using SSM Session Manager. Resource Management: Organize resources using Resource Groups and Tagging Strategy. Large-Scale Operations: Execute commands on multiple servers simultaneously without logging into each machine. Patching: Automate security patch update process. Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T7.1 2 Tagging - Tag Audit: - Review and apply standard tags to all resources - Format: Env:Dev, Project:FCJ, Owner:Student 10/20/2025 10/20/2025 Completed https://cloudjourney.awsstudygroup.com/ T7.2 2 SSM - Role Update: - Update EC2 IAM Role - Add policy AmazonSSMManagedInstanceCore 10/20/2025 10/21/2025 Completed https://cloudjourney.awsstudygroup.com/ T7.3 3 SSM - Session Manager: - Access EC2 Instance via Console - Use Session Manager instead of SSH 10/21/2025 10/22/2025 Completed https://cloudjourney.awsstudygroup.com/ T7.4 4 Security - Hardening: - Remove Port 22 Rule in Security Group Web-SG - Verify access via SSM (success) - Verify via SSH (fail as expected) 10/22/2025 10/23/2025 Completed https://cloudjourney.awsstudygroup.com/ T7.5 5 Resource Groups - Grouping: - Create Resource Group based on tag Project:FCJ - Centrally manage resources 10/23/2025 10/24/2025 Completed https://cloudjourney.awsstudygroup.com/ T7.6 6 SSM - Run Command: - Use Run Command - Run yum update -y on all instances with Env:Dev 10/24/2025 10/25/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 7 Achievements: Enhanced Security:\nSignificantly reduced Attack Surface No admin ports exposed to Internet All access sessions logged in CloudTrail and Session Manager logs Can record sessions for audit Operational Efficiency:\nUpdated software for entire Autoscaling Group with just a few clicks No need to manage SSH keys for each user Can run scripts on hundreds of servers simultaneously Mindset:\nShifted from \u0026ldquo;Managing individual servers\u0026rdquo; to \u0026ldquo;Fleet Management\u0026rdquo; Understanding of Zero Trust Network Access No need for bastion host or VPN Skills:\nProficient in SSM Session Manager Understanding of IAM Instance Profile Know how to organize resources with Tags and Resource Groups Use SSM Run Command for automation "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.8-week8/","title":"Week 8 Worklog","tags":[],"description":"","content":"Week 8 Objectives: Infrastructure Automation: Recreate network architecture (VPC) and servers (EC2) with code. Understanding Essence: Master CloudFormation structure (Parameters, Resources, Outputs). Modern Tools: Get familiar with AWS CDK and the cdk init, cdk synth, cdk deploy workflow. Lifecycle Management: Practice clean deletion (Destroy) and recreation (Deploy) of entire stack in minutes. Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T8.1 2 CloudFormation - Write Template: - Write vpc.yaml file defining VPC - Include: Subnet, Internet Gateway, Route Table 10/27/2025 10/27/2025 Completed https://cloudjourney.awsstudygroup.com/ T8.2 2 CloudFormation - Deploy Stack: - Upload file to CloudFormation Console - Create stack FCJ-VPC-Stack - Verify resources created 10/27/2025 10/28/2025 Completed https://cloudjourney.awsstudygroup.com/ T8.3 3 CDK - Init Project: - Install AWS CDK - Initialize TypeScript project: cdk init app --language typescript 10/28/2025 10/28/2025 Completed https://cloudjourney.awsstudygroup.com/ T8.4 4 CDK - Define S3: - In lib/stack.ts, add code to create S3 Bucket - Enable versioning and encryption (L2 Construct) 10/29/2025 10/29/2025 Completed https://cloudjourney.awsstudygroup.com/ T8.5 5 CDK - Deploy: - Run cdk deploy - Observe ChangeSet creation and execution 10/30/2025 10/30/2025 Completed https://cloudjourney.awsstudygroup.com/ T8.6 5 IaC - Drift Detection: - Manually change S3 bucket tag on Console - Run Drift Detection to detect discrepancy 10/30/2025 10/31/2025 Completed https://cloudjourney.awsstudygroup.com/ T8.7 6 IaC - Cleanup: - Run cdk destroy to remove all resources - Ensure no cost leakage 10/31/2025 10/31/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 8 Achievements: Speed:\nCan rebuild entire network environment in just 2 minutes running commands Instead of 30 minutes of manual clicking Minimizes human errors Control:\nInfrastructure code stored in Git Review change history (Who modified the Subnet? Why?) Code review for infrastructure changes Version control for infrastructure Experience:\nCDK is intuitive and requires less code than pure CloudFormation Thanks to high-level Constructs (L2, L3) Has type checking and autocomplete Easy to test infrastructure code Skills:\nUnderstanding of Infrastructure as Code (IaC) Solid grasp of CloudFormation template structure Proficient in AWS CDK with TypeScript Know how to use Drift Detection Used AWS CLI to perform basic operations such as:\nCheck account \u0026amp; configuration information Retrieve the list of regions View EC2 service Create and manage key pairs Check information about running services \u0026hellip; Acquired the ability to connect between the web interface and CLI to manage AWS resources in parallel.\n\u0026hellip;\n"},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.9-week9/","title":"Week 9 Worklog","tags":[],"description":"","content":"Week 9 Objectives: Network Transparency: Collect and analyze network traffic to detect unusual connections. Log Querying: Use CloudWatch Logs Insights to run queries on massive log data. Cost Optimization: Identify wasted resources and perform \u0026ldquo;Right Sizing\u0026rdquo;. Billing Permissions: Configure Billing access permissions for IAM accounts. Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T9.1 2 VPC - Enable Flow Logs: - Enable Flow Logs for VPC - Destination: CloudWatch Logs Group /aws/vpc/flowlogs 11/03/2025 11/03/2025 Completed https://cloudjourney.awsstudygroup.com/ T9.2 2 Testing - Generate Deny: - Try SSH from IP not in Security Group - Generate REJECT records 11/03/2025 11/04/2025 Completed https://cloudjourney.awsstudygroup.com/ T9.3 3 Logs - Insights Query: - Write query to count rejected packets by Source IP - filter action=\u0026quot;REJECT\u0026quot; stats count() by srcAddr 11/04/2025 11/04/2025 Completed https://cloudjourney.awsstudygroup.com/ T9.4 4 Cost - Compute Optimizer: - Access Compute Optimizer - View recommendations (requires at least 12-24h of data) 11/05/2025 11/05/2025 Completed https://cloudjourney.awsstudygroup.com/ T9.5 5 Cost - Cost Explorer: - Analyze cost by day and service - Group by Service - Identify whether EC2 or RDS costs most 11/06/2025 11/06/2025 Completed https://cloudjourney.awsstudygroup.com/ T9.6 6 Report \u0026amp; Recommendations: - Compile cost report and optimization recommendations - Analyze Flow Logs to find security issues - Propose architecture improvements 11/07/2025 11/07/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 9 Achievements: Investigation:\nIdentified which IP addresses are attempting to scan SSH port Discovered attack patterns Can trace network path Savings:\nCompute Optimizer indicated instances running below 5% CPU Confirmed t3.micro is appropriate or can be consolidated Discovered unused resources Skills:\nProficient in Logs Insights query syntax Critical skill for rapid troubleshooting Understanding of VPC Flow Logs format FinOps:\nKnow how to analyze costs from multiple dimensions Understanding of Cost Allocation Tags Can forecast monthly costs "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.10-week10/","title":"Week 10 Worklog","tags":[],"description":"","content":"Week 10 Objectives: Build API: Create RESTful API for a Book Store management application. Serverless Logic: Write Lambda functions to perform CRUD (Create, Read, Update, Delete). NoSQL Storage: Design high-performance DynamoDB table. Integration: Connect API Gateway -\u0026gt; Lambda -\u0026gt; DynamoDB. Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T10.1 2 DynamoDB - Create Table: - Create Books table with Partition Key ISBN (String) - Configure On-Demand Capacity mode 11/10/2025 11/10/2025 Completed https://cloudjourney.awsstudygroup.com/ T10.2 2 IAM - Lambda Role: - Create Role for Lambda - Permissions: PutItem, GetItem, Scan on Books table - CloudWatch log write permission 11/10/2025 11/11/2025 Completed https://cloudjourney.awsstudygroup.com/ T10.3 3 Lambda - Function Logic: - Write add_book function (Python) to receive JSON and write to DynamoDB - Write get_books function to read data 11/11/2025 11/12/2025 Completed https://cloudjourney.awsstudygroup.com/ T10.4 4 API Gateway - REST API: - Create API BookStoreAPI - Create resource /books and methods POST, GET - Integrate with Lambda functions 11/12/2025 11/13/2025 Completed https://cloudjourney.awsstudygroup.com/ T10.5 5 API Gateway - Deploy: - Deploy API to Stage dev - Get Invoke URL 11/13/2025 11/14/2025 Completed https://cloudjourney.awsstudygroup.com/ T10.6 6 Testing - Postman: - Send POST request to add book - Send GET request to verify returned list 11/14/2025 11/14/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 10 Achievements: Product:\nA complete operational backend API No servers needed (No EC2) Truly serverless architecture Performance:\nExtremely fast response time (\u0026lt; 100ms) Can handle thousands of requests/second by default Auto scaling without configuration Cost:\nNearly $0 during development phase Thanks to Lambda and DynamoDB Free Tier Pay-per-request model Architecture:\nUnderstanding of Microservices architecture Event-driven programming API-first design NoSQL data modeling "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.11-week11/","title":"Week 11 Worklog","tags":[],"description":"","content":"Week 11 Objectives: Architecture Audit: Review entire infrastructure based on AWS Well-Architected Tool. Quota Management: Check Service Quotas and understand the process for requesting quota increases. Resource Cleanup: Find and delete \u0026ldquo;orphaned\u0026rdquo; resources. Advanced Budgets: Set up AWS Budgets with forecasted breach alerts. Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T11.1 2 Governance - Quota Check: - Check vCPU quota for instance family - Running On-Demand Standard instances 11/17/2025 11/17/2025 Completed https://cloudjourney.awsstudygroup.com/ T11.2 2 Cost - Budget Forecast: - Create Budget to alert if forecast exceeds $10 by month end - Instead of waiting for breach to alert 11/17/2025 11/18/2025 Completed https://cloudjourney.awsstudygroup.com/ T11.3 3 WAF - Tool Review: - Open AWS Well-Architected Tool - Create new Workload - Answer questions in Security pillar 11/18/2025 11/18/2025 Completed https://cloudjourney.awsstudygroup.com/ T11.4 4 Cleanup - EBS Audit: - Find EBS Volumes with Available status - Delete to reduce costs 11/19/2025 11/19/2025 Completed https://cloudjourney.awsstudygroup.com/ T11.5 5 Cleanup - EIP Audit: - Release Elastic IPs not attached to any instance - AWS charges penalty for unused EIPs 11/20/2025 11/20/2025 Completed https://cloudjourney.awsstudygroup.com/ T11.6 6 Documentation \u0026amp; Best Practices: - Compile Well-Architected Review documentation - Write improvement recommendation report - Update architecture diagram with findings 11/21/2025 11/21/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 11 Achievements: Risk Report:\nWell-Architected Tool indicated High Risk Issue Week 4 Database running Single-AZ If AZ fails, DB loses connection Need to consider Multi-AZ for production Optimization:\nDeleted 2 leftover 10GB EBS volumes Save approximately $2/month Released 1 unused Elastic IP Avoid penalty fee of $3.6/month Awareness:\n\u0026ldquo;Good architecture\u0026rdquo; is a continuous process Not a destination Need regular review and improvement Well-Architected Framework is the guiding principle Governance:\nUnderstanding of Service Quotas and Limits Know how to request quota increase Can forecast when scaling is needed Proactive capacity planning "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/1-worklog/1.12-week12/","title":"Week 12 Worklog","tags":[],"description":"","content":"Week 12 Objectives: Knowledge Synthesis: Deploy capstone final project combining IaaS and PaaS. Certification: Complete Mock Exam for SAA-C03 certification with score \u0026gt;70%. Capability Portfolio: Update CV, GitHub Profile with completed projects. Cleanup: Terminate all resources to avoid post-course charges. Tasks to be carried out this week: Task ID Day Work Start Date Completion Date Status Reference Material T12.1 2 Capstone - Architecture: - Draw architecture diagram for final project (AWS Icons) - Include: ALB, ASG, RDS, S3, CloudFront 11/24/2025 11/24/2025 Completed https://cloudjourney.awsstudygroup.com/ T12.2 3 Capstone - Deployment: - Deploy project using CDK or CloudFormation - Ensure application runs well, DB connection successful 11/25/2025 11/26/2025 Completed https://cloudjourney.awsstudygroup.com/ T12.3 4 Career - Community: - Join \u0026ldquo;AWS Study Group\u0026rdquo; on Facebook/LinkedIn - Connect with FCJ Workforce mentors 11/26/2025 11/27/2025 Completed https://cloudjourney.awsstudygroup.com/ T12.4 5 Exam - Practice Test: - Complete 65-question test in 130 minutes (simulating actual exam) - Review incorrect answers 11/27/2025 11/28/2025 Completed https://cloudjourney.awsstudygroup.com/ T12.5 6 Cleanup - NUKE: - Use aws-nuke tool (or manual deletion) - Clean account - Verify in all Regions 11/28/2025 11/30/2025 Completed https://cloudjourney.awsstudygroup.com/ Week 12 Achievements: Product:\nA GitHub Repository containing CDK code Standard 3-Tier architecture Complete documentation README with architecture diagram Confidence:\nReady for AWS SAA-C03 certification exam Ready for Cloud internship interviews Have portfolio to showcase Clear understanding of core AWS services Completion:\nClean AWS account No ongoing charges Backed up all important artifacts Ready for next journey Journey:\nFrom \u0026ldquo;knowing nothing\u0026rdquo; about Cloud To genuine \u0026ldquo;AWS Builder\u0026rdquo; 12 weeks = 84 days of transformation Solid foundation for Cloud Engineer career "},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/categories/","title":"Categories","tags":[],"description":"","content":""},{"uri":"https://github.com/LeThiYenVi/AWS-Report.git/tags/","title":"Tags","tags":[],"description":"","content":""}]